{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Méta: paramètres, to-do list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Questions:\n",
    "- [ ] Les tokens texte et les tokens images sont identifiés par les mêmes entiers. Est-ce problématique? Quelle alternative avons-nous?\n",
    "- [ ] \"bart-large\" est limité à des séquences de 1024 tokens, vérifier pour bart-large-cnn (dans le code, il y a un bug avec 1025 tokens = 1 token \"begining of sentence\" + 1024 image tokens)\n",
    "\n",
    "\n",
    "Must-have :\n",
    "- [X] Encoder le texte avec Bart\n",
    "- [X] Encoder les images avec Dall-E dVAE \n",
    "- [X] Une étape d'apprentissage avec Bart\n",
    "- [ ] Une séquence d'apprentissage sur un mini-batch d'exemples provenant de Coco\n",
    "- [ ] Un print des progrès de l'apprentissage\n",
    "- [ ] Une prédiction d'image avec le decoder Dall-E dVAE\n",
    "\n",
    "Nice-to-have:\n",
    "- [ ] Passer tous les paramètres en début de notebook\n",
    "- [X] Sauvegarder les modèles localement pour ne pas les télécharger à chaque fois\n",
    "- [ ] Voir si possible d'utiliser GPT-3 à la place de Bart\n",
    "- [ ] Utiliser les TPU sur Google Collab en activant l'offre envoyée à Arthur\n",
    "- [ ] Uniformiser les noms de variable\n",
    "- [ ] Utiliser un plus petit modèle que Bart CNN \n",
    "- [ ] Gérer l'attention du transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "0d5cc053-1ec9-45ed-a73f-4258868782c6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 78798487,
    "execution_start": 1644223934687,
    "source_hash": "39dd11f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = 2\n",
    "text_token_length = 255\n",
    "target_image_size = 256\n",
    "image_token_side = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c7099527-3f4c-498d-b871-ec1c1fd361fd",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 1. Entraînement\n",
    "\n",
    "L'entraînement consiste en:\n",
    "\n",
    "1. Encoder le texte en tokens-text\n",
    "2. Encoder les images en tokens-image\n",
    "3. Modéliser l'ensemble de façon auto-régressive\n",
    "\n",
    "### 1.1. Encodage du texte\n",
    "\n",
    "Nous utilison `BartTokenizer` pour l'encodage du texte comme mini Dall-E. Le Dall-E original utilise selon l'article du \"_BPE-encoding_\" (byte-pair encoding, c'est à dire strictement parlant des paires de caractères), ce qui peut s'interpréter comme l'utilisation du modèle GPT-3, qui repose lui-aussi sur un encodage proche d'un _BPE encoding_. Malheureusement, GPT-3 n'est pas disponible au grand public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://huggingface.co/facebook/bart-large-cnn ../../models/facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cell_id": "4960c543-3ac9-4670-a024-15323b7baf1c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4984,
    "execution_start": 1644223934687,
    "source_hash": "9d1ba821",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption is rendered as tokens by:\n",
      "tensor([[    0,   250, 31918, 31526,   179,  2934,    15,     5,  2480,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "\n",
    "# https://huggingface.co/transformers/v2.11.0/model_doc/bart.html\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\n",
    "    '../../models/facebook/bart-large-cnn'\n",
    ")\n",
    "\n",
    "caption = \"A Emperor penguin standing on the ice\"\n",
    "\n",
    "# First version, taht does not generalize to a list of captions\n",
    "# It returns an object: {input_ids:..., attention_mask: ...}\n",
    "# caption_as_tokens = tokenizer(caption)\n",
    "\n",
    "caption_as_tokens = tokenizer.encode(\n",
    "    caption,\n",
    "    max_length = text_token_length,\n",
    "    padding = 'max_length',\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "# for more than one caption\n",
    "# caption_as_tokens = tokenizer.batch_encode_plus([caption])\n",
    "    \n",
    "if verbose >= 2:\n",
    "    print(\"Caption is rendered as tokens by:\")\n",
    "    print(caption_as_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a6b11683-82ba-495b-a973-c94d0a228332",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 1.2 Encodage des images\n",
    "\n",
    "Pour l'encodage des images, nous utilisons le modèle (dVAE) mis à disposition par Open AI pour Dall-E. L'encodage des images nécessite quelques étape de pré-traitement, par exemple pour le mettres toutes au même format (carré) et à la même taille (256x256 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download encoder.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dall-E dVAE encoder has a meta-pixel look-up table of size:\n",
      "8192\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dall_e import load_model\n",
    "\n",
    "dev = torch.device('cpu')\n",
    "encoder = load_model(\"../../models/openai/dall-e/encoder.pkl\", dev)\n",
    "\n",
    "if verbose>=1:\n",
    "    print(\"Dall-E dVAE encoder has a meta-pixel look-up table of size:\")\n",
    "    print(encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dall_e import map_pixels\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# scale images down to 256x256 (cropping the uneven dimension)\n",
    "# we might get problems with some images from the COCO datasets\n",
    "# ignore these images as a first approximation\n",
    "# or reduce the image resolution ?\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "# replace by direct reading from disk\n",
    "# persist images to disk in the first place\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_id": "7dfe3d91-80d9-445c-a633-ca4e528e202f",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     463,
     194
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8143,
    "execution_start": 1644224111861,
    "source_hash": "f60d5470",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "image = preprocess(download_image(\n",
    "    'https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'\n",
    "))\n",
    "\n",
    "# lines represent the 32*32 image-tokens or meta-pixels\n",
    "# columns represent the 8192 possible meta-pixel values from the look-up table\n",
    "image_as_token_logits = encoder(image)\n",
    "\n",
    "# now we retain the most probable meta-pixel value\n",
    "image_as_tokens = torch.argmax(image_as_token_logits, axis=1)\n",
    "image_as_tokens = image_as_tokens.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Modélisation auto-régressive avec Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 7522,  741,  ..., 5016, 1144, 1005]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hstack((\n",
    "        # prepend begining-of-sentence (BOS) token\n",
    "        torch.tensor(model.config.bos_token_id).repeat(1).unsqueeze(1),\n",
    "        image_as_tokens\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    '../../models/facebook/bart-large-cnn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des prédictions:\n",
      "torch.Size([1, 101, 50264])\n",
      "vs. taille du dictionnaire de tokens-texte:\n",
      "50265\n",
      "vs. taille du dictionnaire de tokens-images:\n",
      "8192\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predict = model(\n",
    "    input_ids = caption_as_tokens,\n",
    "    decoder_input_ids = torch.hstack((\n",
    "        # prepend begining-of-sentence (BOS) token\n",
    "        # and stop after 100 tokens because not\n",
    "        # possible to reach the 1024 image-tokens :()\n",
    "        # works for sizes a number of \n",
    "        torch.tensor(model.config.bos_token_id)\n",
    "          .repeat(image_as_tokens.shape[0])\n",
    "          .unsqueeze(1),\n",
    "        image_as_tokens[:,:100]\n",
    "    ))\n",
    ")\n",
    "\n",
    "# ici nous avons un problème car les prédictions\n",
    "# sont de la taille du dictionnaire de tokens-texte\n",
    "# pas du dictionnaire de tokens-images\n",
    "if verbose >= 1:\n",
    "    print(\"Taille des prédictions:\")\n",
    "    print(predict.logits.shape)\n",
    "    print(\"vs. taille du dictionnaire de tokens-texte:\")\n",
    "    print(tokenizer.vocab_size)\n",
    "    print(\"vs. taille du dictionnaire de tokens-images:\")\n",
    "    print(encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une possibilité est de changer la dernière couche du modèle pour prédire des valeurs du dictionnaire de tokens-image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=50264, bias=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = torch.nn.Linear(\n",
    "    in_features=1024, out_features = encoder.vocab_size,\n",
    "    bias=False\n",
    ")\n",
    "# for some reason, biases are stored elsewhere:\n",
    "model.final_logits_bias = torch.rand(encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model(\n",
    "    input_ids = caption_as_tokens,\n",
    "    decoder_input_ids = torch.hstack((\n",
    "        # prepend begining-of-sentence (BOS) token\n",
    "        # and stop after 100 tokens because not\n",
    "        # possible to reach the 1024 image-tokens :(\n",
    "        # works for sizes a number of \n",
    "        torch.tensor(model.config.bos_token_id)\n",
    "          .repeat(image_as_tokens.shape[0])\n",
    "          .unsqueeze(1),\n",
    "        image_as_tokens[:,:100]\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 101, 8192])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.logits.shape # yes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant comment entraîner ce modèle à l'aide d'une seule image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2402, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict image\n",
    "\n",
    "# 1) same as above\n",
    "predictions = model(\n",
    "    input_ids = caption_as_tokens,\n",
    "    decoder_input_ids = torch.hstack((\n",
    "        torch.tensor(model.config.bos_token_id)\n",
    "          .repeat(image_as_tokens.shape[0])\n",
    "          .unsqueeze(1),\n",
    "        image_as_tokens[:,:100]\n",
    "    ))\n",
    ")\n",
    "# 2) get the predicted next image tokens\n",
    "image_prediction_as_tokens = predictions.logits.argmax(axis=2) # best response\n",
    "# we actually do not need this for the next step and can directly\n",
    "# use logits in the loss function\n",
    "\n",
    "# 3) compare to original\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# la perte est calculée pour chaque paire de tokens (vrai ; prédit)\n",
    "# puis est moyennée sur l'ensemble du vecteur\n",
    "loss = loss_fn(\n",
    "  input  = predictions.logits[0,:,:],\n",
    "  target = image_as_tokens[0,:101]\n",
    ")\n",
    "\n",
    "loss\n",
    "\n",
    "#image_as_tokens[:,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors for true image tokens and predicted image tokens are of size:\n",
      "torch.Size([101])\n",
      "torch.Size([101])\n"
     ]
    }
   ],
   "source": [
    "if verbose >=2 :\n",
    "    print(\"Tensors for true image tokens and predicted image tokens are of size:\")\n",
    "    print(image_as_tokens[0,:101].shape)\n",
    "    print(image_prediction_as_tokens[0,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tokens</th>\n",
       "      <th>predicted_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7522</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>4968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5973</td>\n",
       "      <td>3921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7663</td>\n",
       "      <td>1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>708</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1861</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>563</td>\n",
       "      <td>4572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>435</td>\n",
       "      <td>4947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5677</td>\n",
       "      <td>3663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>7870</td>\n",
       "      <td>2387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     original_tokens  predicted_tokens\n",
       "0               7522               498\n",
       "1                741              4968\n",
       "2               5973              3921\n",
       "3               7663              1932\n",
       "4                708              1039\n",
       "..               ...               ...\n",
       "96              1861              1413\n",
       "97               563              4572\n",
       "98               435              4947\n",
       "99              5677              3663\n",
       "100             7870              2387\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data = {\n",
    "    \"original_tokens\" : image_as_tokens[0,:101],\n",
    "    \"predicted_tokens\" : image_prediction_as_tokens[0,:],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_as_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons optimiser les poids, à l'aide de la fonction de perte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce que la perte a diminué? (Spoiler: oui.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7003, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(\n",
    "    input_ids = caption_as_tokens,\n",
    "    decoder_input_ids = torch.hstack((\n",
    "        torch.tensor(model.config.bos_token_id)\n",
    "          .repeat(image_as_tokens.shape[0])\n",
    "          .unsqueeze(1),\n",
    "        image_as_tokens[:,:100]\n",
    "    ))\n",
    ")\n",
    "\n",
    "loss_fn(\n",
    "  input  = predictions.logits[0,:,:],\n",
    "  target = image_as_tokens[0,:101]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a527766d-88e2-4e72-b628-9e721be47893",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21.1875,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1644224120010,
    "source_hash": "63e1fdf3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre\n",
    "\n",
    "\n",
    "#image_as_tokens_shifted = image_as_tokens.clone()\n",
    "#image_as_tokens_shifted[1:] = image_as_tokens[:-1]\n",
    "#image_as_tokens_shifted[0]  = model.config.decoder_start_token_id\n",
    "\n",
    "\n",
    "#z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# pad text to fixed length with an additional id and bind text and image tokens together\n",
    "\n",
    "# model the sequence with a transformer model\n",
    "\n",
    "\n",
    "# def shift_tokens_right(input_ids: np.array, decoder_start_token_id: int):\n",
    "#     \"\"\"\n",
    "#     Shift input ids one token to the right.\n",
    "#     \"\"\"\n",
    "#     shifted_input_ids = np.zeros(input_ids.shape)\n",
    "#     shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "#     shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "#     return shifted_input_ids\n",
    "\n",
    "\n",
    "    # dataset.preprocess(\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    #     normalize_text=model.config.normalize_text,\n",
    "    #     max_length=model.config.max_text_length,\n",
    "    # )\n",
    "\n",
    "\n",
    "# all_tokens =  torch.cat( (text_tokens,image_tokens) )\n",
    "\n",
    "# if verbose > 2:\n",
    "#     print(all_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1e027c6c-3e79-4dc3-bb7b-bd75685faa11",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 20633,
    "execution_start": 1644224125569,
    "source_hash": "a61b083d",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# loss = cross-entropy\n",
    "# torch.nn.crossEntropy()\n",
    "# predict vs. image_tokens\n",
    "\n",
    "# alternativement on peut changer la dernière couche de Bart\n",
    "# nn.Linear(size_embedding, num_vocab_img)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(RNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "#         self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "#         self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "#     def forward(self, category, input, hidden):\n",
    "#         input_combined = torch.cat((category, input, hidden), 1)\n",
    "#         hidden = self.i2h(input_combined)\n",
    "#         output = self.i2o(input_combined)\n",
    "#         output_combined = torch.cat((hidden, output), 1)\n",
    "#         output = self.o2o(output_combined)\n",
    "#         output = self.dropout(output)\n",
    "#         output = self.softmax(output)\n",
    "#         return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b4bd6a92-d575-4600-9da6-bd86ce8e6a1b",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21
    ],
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 5,
    "execution_start": 1643560602186,
    "source_hash": "f52aae96",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7522,  741, 5973,  ..., 6231, 5016, 1144])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tokens[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "952dbbbe-276e-4de9-b7d9-9510beb5fd9e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7e06b9bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## INFERENCE\n",
    "\n",
    "# get token ids for texts (= encode)\n",
    "\n",
    "# generate the next terms in the sequence with a random seed\n",
    "\n",
    "# get image from token ids (= decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4a4c9072-b41a-402f-99cf-81eb24daa989",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b9555d0b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/14oChMr8KZVS7DzcbsuJix0JQKUTGO64j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "# see ``examples/summarization/bart/evaluate_cnn.py`` for a longer example\n",
    "model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
    "inputs = tokenizer.batch_encode_plus([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4f3692ed-5f27-49a4-899a-82a03e72232c' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "14ce6527-c9b6-497a-b824-14622d8e85eb",
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
