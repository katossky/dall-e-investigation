{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 1. Chargement des données d'entraînement\n\nLes images sont issues de COCO, et ont été filtrées pour (tenter de) ne retenir que des images représentant des paysages.",
   "metadata": {
    "cell_id": "40ab0b08-a7bc-4a8a-9736-8df99ecc1f61",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f37a61e6-a659-4b96-a359-f5bda1426c6c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8307e0de",
    "execution_start": 1644223934686,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "# !wget https://minio.lab.sspcloud.fr/cthiounn2/archive_val.zip\n# !unzip -o ../image_data/archive_val.zip -d ../image_data/",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0d5cc053-1ec9-45ed-a73f-4258868782c6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "39dd11f2",
    "execution_start": 1644223934687,
    "execution_millis": 78798487,
    "deepnote_cell_type": "code"
   },
   "source": "verbose = 0\ntext_token_length = 255\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Entraînement\n\nL'entraînement consiste en:\n\n1. Encoder le texte en tokens-text\n2. Encoder les images en tokens-image\n3. Modéliser l'ensemble de façon auto-régressive\n\n### 1.1. Encodage du texte\n\nNous utilison `BartTokenizer` pour l'encodage du texte comme mini Dall-E. Le Dall-E original utilise selon l'article du \"_BPE-encoding_\" (byte-pair encoding, c'est à dire strictement parlant des paires de caractères), ce qui peut s'interpréter comme l'utilisation du modèle GPT-3, qui repose lui-aussi sur un encodage proche d'un _BPE encoding_. Malheureusement, GPT-3 n'est pas disponible au grand public.",
   "metadata": {
    "cell_id": "c7099527-3f4c-498d-b871-ec1c1fd361fd",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4960c543-3ac9-4670-a024-15323b7baf1c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9d1ba821",
    "execution_start": 1644223934687,
    "execution_millis": 4984,
    "deepnote_cell_type": "code"
   },
   "source": "# TO DO:\n# - put all the parameters at the begning of the \n# - save the weights locally to avoid download each time\n# - see if possible to use GPT-3 with Open-AI free account\n# - enable TPUs usage offered by Google\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nimport torch\n\n# https://huggingface.co/transformers/v2.11.0/model_doc/bart.html\n\ncaption = \"A Emperor penguin standing on the ice\"\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn') # essayer une taille plus faible\n\ntext_tokens = tokenizer(caption, max_length=text_token_length, padding='max_length')['input_ids']\ntext_tokens = torch.as_tensor(text_tokens) # BPE-encoding\nif verbose >= 2:\n    print(text_tokens)\n",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b592c48a9cc4467abef1937cdade2f92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c027503f85bf4c9bb84ce2b3cf049f11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d3cf971063c4ef0a6bc259354767f5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65e89772c1a942c5bfce3d295c46a629"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 1.2 Encodage des images\n\nNous utilisons alternativement ",
   "metadata": {
    "cell_id": "a6b11683-82ba-495b-a973-c94d0a228332",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1ea40b2c-3ba9-4905-8202-817bdbc14577",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b7150b3d",
    "execution_start": 1644223983219,
    "execution_millis": 5669,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install dall_e",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: dall_e in /root/venv/lib/python3.7/site-packages (0.1)\nRequirement already satisfied: mypy in /root/venv/lib/python3.7/site-packages (from dall_e) (0.931)\nRequirement already satisfied: numpy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (1.19.5)\nRequirement already satisfied: requests in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (2.26.0)\nRequirement already satisfied: torchvision in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (0.11.1)\nRequirement already satisfied: torch in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (1.10.0)\nRequirement already satisfied: blobfile in /root/venv/lib/python3.7/site-packages (from dall_e) (1.2.8)\nRequirement already satisfied: Pillow in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (8.4.0)\nRequirement already satisfied: pytest in /shared-libs/python3.7/py/lib/python3.7/site-packages (from dall_e) (6.2.5)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from mypy->dall_e) (0.4.3)\nRequirement already satisfied: typed-ast<2,>=1.4.0; python_version < \"3.8\" in /root/venv/lib/python3.7/site-packages (from mypy->dall_e) (1.5.2)\nRequirement already satisfied: tomli>=1.1.0 in /root/venv/lib/python3.7/site-packages (from mypy->dall_e) (2.0.0)\nRequirement already satisfied: typing-extensions>=3.10 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from mypy->dall_e) (4.0.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->dall_e) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->dall_e) (2021.10.8)\nRequirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->dall_e) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->dall_e) (3.3)\nRequirement already satisfied: filelock~=3.0 in /root/venv/lib/python3.7/site-packages (from blobfile->dall_e) (3.4.2)\nRequirement already satisfied: xmltodict~=0.12.0 in /root/venv/lib/python3.7/site-packages (from blobfile->dall_e) (0.12.0)\nRequirement already satisfied: pycryptodomex~=3.8 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from blobfile->dall_e) (3.12.0)\nRequirement already satisfied: py>=1.8.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pytest->dall_e) (1.11.0)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pytest->dall_e) (1.0.0)\nRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pytest->dall_e) (4.8.2)\nRequirement already satisfied: toml in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pytest->dall_e) (0.10.2)\nRequirement already satisfied: packaging in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pytest->dall_e) (21.3)\nRequirement already satisfied: iniconfig in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pytest->dall_e) (1.1.1)\nRequirement already satisfied: attrs>=19.2.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pytest->dall_e) (21.2.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->dall_e) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from packaging->pytest->dall_e) (3.0.6)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.3 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "7dfe3d91-80d9-445c-a633-ca4e528e202f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f60d5470",
    "execution_start": 1644224111861,
    "execution_millis": 8143,
    "deepnote_output_heights": [
     463,
     194
    ],
    "deepnote_cell_type": "code"
   },
   "source": "## TRAINING \nimport io\nimport os, sys\nimport requests\nimport PIL\n\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport torch.nn.functional as F\n\nfrom dall_e          import map_pixels, unmap_pixels, load_model\nfrom IPython.display import display, display_markdown\n\ndev = torch.device('cpu')\n\n\n# get token ids for images (= encode)\n\nencoder = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", dev)\n\n\ntarget_image_size = 256\n\n# scale images down to 256x256 (cropping the uneven dimension)\n# we might get problems with some images from the COCO datasets\n# ignore these images as a first approximation\n# or reduce the image resolution ?\n\ndef preprocess(img):\n    s = min(img.size)\n    \n    if s < target_image_size:\n        raise ValueError(f'min dim for image {s} < {target_image_size}')\n        \n    r = target_image_size / s\n    s = (round(r * img.size[1]), round(r * img.size[0]))\n    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n    img = TF.center_crop(img, output_size=2 * [target_image_size])\n    img = torch.unsqueeze(T.ToTensor()(img), 0)\n    return map_pixels(img)\n\n# replace by direct reading from disk\n# persist images to disk in the first place\ndef download_image(url):\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return PIL.Image.open(io.BytesIO(resp.content))\n\nx = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n\nz_logits = encoder(x)\nz = torch.argmax(z_logits, axis=1)\n#z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n\n# pad text to fixed length with an additional id and bind text and image tokens together\n\n# model the sequence with a transformer model\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a527766d-88e2-4e72-b628-9e721be47893",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "63e1fdf3",
    "execution_start": 1644224120010,
    "execution_millis": 3,
    "deepnote_output_heights": [
     21.1875,
     21.1875
    ],
    "deepnote_cell_type": "code"
   },
   "source": "\nimage_tokens = z.flatten()\n\n# def shift_tokens_right(input_ids: np.array, decoder_start_token_id: int):\n#     \"\"\"\n#     Shift input ids one token to the right.\n#     \"\"\"\n#     shifted_input_ids = np.zeros(input_ids.shape)\n#     shifted_input_ids[:, 1:] = input_ids[:, :-1]\n#     shifted_input_ids[:, 0] = decoder_start_token_id\n#     return shifted_input_ids\n\n\n    # dataset.preprocess(\n    #     tokenizer=tokenizer,\n    #     decoder_start_token_id=model.config.decoder_start_token_id,\n    #     normalize_text=model.config.normalize_text,\n    #     max_length=model.config.max_text_length,\n    # )\n\n\n# all_tokens =  torch.cat( (text_tokens,image_tokens) )\n\n# if verbose > 2:\n#     print(all_tokens.shape)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e817e222-729b-4cc0-acad-e9cde0626bd8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5d3490be",
    "execution_start": 1644224004676,
    "execution_millis": 1386,
    "deepnote_cell_type": "code"
   },
   "source": "!cd model\n!git clone https://huggingface.co/facebook/bart-large-cnn",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "/bin/bash: line 0: cd: model: No such file or directory\nfatal: destination path 'bart-large-cnn' already exists and is not an empty directory.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1e027c6c-3e79-4dc3-bb7b-bd75685faa11",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a61b083d",
    "execution_start": 1644224125569,
    "execution_millis": 20633,
    "deepnote_cell_type": "code"
   },
   "source": "# QUESTIONS:\n# - Is it a problem that there is potentiel overlap between the indices of image and text-tokens ?\n\nfrom transformers import BartForConditionalGeneration\n\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n# bart-large limité à des séquences de 1024 tokens, vérifier pour bart-large-cnn\n# tester un modèle plus petit que bart-large-cnn\n\nimage_tokens[1:] = image_tokens[:-1]\nimage_tokens[0]  = model.config.decoder_start_token_id\n\npredict = model(\n    input_ids = text_tokens,\n    decoder_input_ids = image_tokens,\n    # attention_mask = torch.ones_like(all_tokens)\n) # retourne un objet de même taille que decoder_input_ids\n\n# loss = cross-entropy\n# torch.nn.crossEntropy()\n# predict vs. image_tokens\n\n# alternativement on peut changer la dernière couche de Bart\n# nn.Linear(size_embedding, num_vocab_img)\n\n# import torch\n# import torch.nn as nn\n# class RNN(nn.Module):\n#     def __init__(self, input_size, hidden_size, output_size):\n#         super(RNN, self).__init__()\n#         self.hidden_size = hidden_size\n#         self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n#         self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n#         self.o2o = nn.Linear(hidden_size + output_size, output_size)\n#         self.dropout = nn.Dropout(0.1)\n#         self.softmax = nn.LogSoftmax(dim=1)\n#     def forward(self, category, input, hidden):\n#         input_combined = torch.cat((category, input, hidden), 1)\n#         hidden = self.i2h(input_combined)\n#         output = self.i2o(input_combined)\n#         output_combined = torch.cat((hidden, output), 1)\n#         output = self.o2o(output_combined)\n#         output = self.dropout(output)\n#         output = self.softmax(output)\n#         return output, hidden",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b4bd6a92-d575-4600-9da6-bd86ce8e6a1b",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "f52aae96",
    "execution_start": 1643560602186,
    "execution_millis": 5,
    "deepnote_output_heights": [
     21
    ],
    "deepnote_cell_type": "code"
   },
   "source": "image_tokens[:-1]",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 8,
     "data": {
      "text/plain": "tensor([7522,  741, 5973,  ..., 6231, 5016, 1144])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "952dbbbe-276e-4de9-b7d9-9510beb5fd9e",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7e06b9bf",
    "deepnote_cell_type": "code"
   },
   "source": "## INFERENCE\n\n# get token ids for texts (= encode)\n\n# generate the next terms in the sequence with a random seed\n\n# get image from token ids (= decode)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4a4c9072-b41a-402f-99cf-81eb24daa989",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b9555d0b",
    "deepnote_cell_type": "code"
   },
   "source": "# https://colab.research.google.com/drive/14oChMr8KZVS7DzcbsuJix0JQKUTGO64j",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4f3692ed-5f27-49a4-899a-82a03e72232c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "14ce6527-c9b6-497a-b824-14622d8e85eb",
  "deepnote_execution_queue": []
 }
}