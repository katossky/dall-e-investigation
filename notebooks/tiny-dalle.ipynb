{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - [x] save the weights locally to avoid download each time\n",
    "# - [ ] back-propagation on one exemple\n",
    "# - [ ] cycle of back-porpagation on a set of exemple\n",
    "# - [ ] metric logging of progress during training\n",
    "# - [ ] GPU distribution of training\n",
    "# - [ ] add attention masks if need be\n",
    "# - [ ] see if possible to use GPT-3 with Open-AI free account\n",
    "# - [ ] dans l'ensemble des documents, reprendre les lettres utilisées par Torch pour plus de clarté: N = nombre d'exemples / observations ; C = nombre de classes (nombre de mots de chaque dictionnaire), cf. https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "40ab0b08-a7bc-4a8a-9736-8df99ecc1f61",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 1. Chargement des données d'entraînement\n",
    "\n",
    "Les images sont issues de COCO, et ont été filtrées pour (tenter de) ne retenir que des images représentant des paysages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f37a61e6-a659-4b96-a359-f5bda1426c6c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7951,
    "execution_start": 1643448375152,
    "source_hash": "d53b0560",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-29 09:26:15--  https://minio.lab.sspcloud.fr/cthiounn2/archive_val.zip\n",
      "Resolving minio.lab.sspcloud.fr (minio.lab.sspcloud.fr)... 185.24.184.229, 185.24.184.228\n",
      "Connecting to minio.lab.sspcloud.fr (minio.lab.sspcloud.fr)|185.24.184.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 85830108 (82M) [binary/octet-stream]\n",
      "Saving to: ‘archive_val.zip’\n",
      "\n",
      "archive_val.zip     100%[===================>]  81.85M  13.9MB/s    in 6.5s    \n",
      "\n",
      "2022-01-29 09:26:22 (12.6 MB/s) - ‘archive_val.zip’ saved [85830108/85830108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://minio.lab.sspcloud.fr/cthiounn2/archive_val.zip\n",
    "# !unzip -o ../image_data/archive_val.zip -d ../image_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "0d5cc053-1ec9-45ed-a73f-4258868782c6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1643464382142,
    "source_hash": "39dd11f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = 0\n",
    "text_token_length = 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c7099527-3f4c-498d-b871-ec1c1fd361fd",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 1. Entraînement\n",
    "\n",
    "L'entraînement consiste en:\n",
    "\n",
    "1. Encoder le texte en tokens-texte\n",
    "2. Encoder les images en tokens-image\n",
    "3. Modéliser l'ensemble de façon auto-régressive\n",
    "\n",
    "### 1.1. Encodage du texte\n",
    "\n",
    "Nous utilison `BartTokenizer` pour l'encodage du texte comme mini Dall-E. Le Dall-E original utilise selon l'article du \"_BPE-encoding_\" (byte-pair encoding, c'est à dire strictement parlant des paires de caractères), ce qui peut s'interpréter comme l'utilisation du modèle GPT-3, qui repose lui-aussi sur un encodage proche d'un _BPE encoding_. Malheureusement, GPT-3 n'est pas disponible au grand public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans '../models/facebook/bart-large-cnn'...\n",
      "remote: Enumerating objects: 75, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 75 (delta 31), reused 0 (delta 0)\u001b[K 342.00 Kio/s\n",
      "Dépaquetage des objets: 100% (75/75), 1.06 Mio | 379.00 Kio/s, fait.\n",
      "^C\n",
      "warning: Le clone a réussi, mais l'extraction a échoué.\n",
      "Vous pouvez inspecter ce qui a été extrait avec 'git status'\n",
      "et réessayer avec 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/facebook/bart-large-cnn ../models/facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "4960c543-3ac9-4670-a024-15323b7baf1c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 523,
    "execution_start": 1643464428289,
    "source_hash": "effcf910",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "\n",
    "# https://huggingface.co/transformers/v2.11.0/model_doc/bart.html\n",
    "\n",
    "caption = \"A Emperor penguin standing on the ice\"\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained('../models/facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "text_tokens = tokenizer(caption, max_length=text_token_length, padding='max_length')['input_ids']\n",
    "text_tokens = torch.as_tensor(text_tokens) # BPE-encoding\n",
    "if verbose >= 2:\n",
    "    print(text_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a6b11683-82ba-495b-a973-c94d0a228332",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 1.2 Encodage des images\n",
    "\n",
    "Nous utilisons alternativement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  205M  100  205M    0     0  1107k      0  0:03:09  0:03:09 --:--:-- 1214k      0  0:02:56  0:00:17  0:02:39 1215k:00:22  0:02:34 1203k32 65.8M    0     0  1074k      0  0:03:15  0:01:02  0:02:13  350k 0:03:26  0:01:13  0:02:13 1196k0  1038k      0  0:03:22  0:01:23  0:01:59 1180k3:19  0:01:29  0:01:50 1218k 0  1079k      0  0:03:14  0:01:50  0:01:24 1193k0:02:15  0:00:55 1212k03:10  0:02:16  0:00:54 1208k     0  1099k      0  0:03:11  0:02:55  0:00:16 1184k0  0:03:00  0:00:10 1174k 0:03:09 --:--:-- 1218k\n"
     ]
    }
   ],
   "source": [
    "# Create a local copy\n",
    "# # ! curl https://cdn.openai.com/dall-e/encoder.pkl --create-dirs -o ../models/openai/dall-e/encoder.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "7dfe3d91-80d9-445c-a633-ca4e528e202f",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     463,
     194
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7654,
    "execution_start": 1643464433128,
    "source_hash": "f60d5470",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## TRAINING \n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dall_e          import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "dev = torch.device('cpu')\n",
    "\n",
    "\n",
    "# get token ids for images (= encode)\n",
    "\n",
    "encoder = load_model(\"../models/openai/dall-e/encoder.pkl\", dev)\n",
    "\n",
    "\n",
    "target_image_size = 256\n",
    "\n",
    "# scale images down to 256x256 (cropping the uneven dimension)\n",
    "# we might get problems with some images from the COCO datasets\n",
    "# ignore these images as a first approximation\n",
    "# or reduce the image resolution ?\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)\n",
    "\n",
    "# replace by direct reading from disk\n",
    "# persist images to disk in the first place\n",
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return PIL.Image.open(io.BytesIO(resp.content))\n",
    "\n",
    "x = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n",
    "\n",
    "z_logits = encoder(x)\n",
    "z = torch.argmax(z_logits, axis=1)\n",
    "#z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# pad text to fixed length with an additional id and bind text\n",
    "# and image tokens together\n",
    "\n",
    "# model the sequence with a transformer model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "a527766d-88e2-4e72-b628-9e721be47893",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21.1875,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1643464448246,
    "source_hash": "28ced6d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "image_tokens = z.flatten()\n",
    "# all_tokens=  torch.cat( (text_tokens,image_tokens) )\n",
    "# \n",
    "# if verbose > 2:\n",
    "#    print(all_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Entraînement avec **une** image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "1e027c6c-3e79-4dc3-bb7b-bd75685faa11",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 557,
    "execution_start": 1643464465189,
    "source_hash": "4a952c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTIONS:\n",
    "# - Is it a problem that there is potentiel overlap between the indices of image and text-tokens ?\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('../models/facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = text_tokens.unsqueeze(0)\n",
    "text_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tokens = image_tokens.unsqueeze(0)\n",
    "image_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model(input_ids = text_tokens, decoder_input_ids = image_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 50264])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.logits.shape # les prédictions sont dans le plongement des tokens-texte, pas des tokens-image !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size # 50265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size # 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle BART utilisé est prévu pour utiliser un seul et même dictionnaire pour faire des résumés de texte. Or ici nous voulons utiliser un dictionnaire nouveau pour le décodage... Il nous faut donc remplacer la dernière couche du modèle, pour prédire non pas la proba du prochain token parmi 50264/5 (?) tokens du vocabulaire de token-texte, mais plutôt la proba du prochain token parmi 8192 tokens-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=50264, bias=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model.lm_head = nn.Linear(in_features=1024, out_features=16384, bias=False)\n",
    "# we just change the prediction size of the last layer\n",
    "model.final_logits_bias = torch.rand(16384)\n",
    "# for some reason, the bias are stored outside the neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 16384])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model(input_ids = text_tokens, decoder_input_ids = image_tokens)\n",
    "predict.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons la précision de ce modèle (jusqu'à présent non-entraîné pour cette tâche spécifique) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "loss = cross_entropy(predict.logits.squeeze(), image_tokens.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8533, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # > 30 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'new_zeros'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-385d2dfcf13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# je ne comprends pas pourquoi la fonction de perte ne change pas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# après appel à la méthode .backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# par ailleurs l'appel à squeeze() ici sous-entend qu'on ne va pas pouvoir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                 )\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# input_ids if no decoder_input_ids are provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdecoder_inputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             decoder_input_ids = shift_tokens_right(\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mshift_tokens_right\u001b[0;34m(input_ids, pad_token_id, decoder_start_token_id)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mShift\u001b[0m \u001b[0minput\u001b[0m \u001b[0mids\u001b[0m \u001b[0mone\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mshifted_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mshifted_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mshifted_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'new_zeros'"
     ]
    }
   ],
   "source": [
    "# je ne comprends pas pourquoi la fonction de perte ne change pas\n",
    "# après appel à la méthode .backward()\n",
    "model.forward()\n",
    "predict = model(input_ids = text_tokens, decoder_input_ids = image_tokens)\n",
    "# par ailleurs l'appel à squeeze() ici sous-entend qu'on ne va pas pouvoir \n",
    "# calculer la fonction de perte en une seule fois sur un batch\n",
    "# ou alors qu'il va falloir ruser...\n",
    "loss = cross_entropy(predict.logits.squeeze(), image_tokens.squeeze())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8533, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b4bd6a92-d575-4600-9da6-bd86ce8e6a1b",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1643464121273,
    "source_hash": "cb739972",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pus tard: gestion de l'attention\n",
    "# torch.ones_like(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "952dbbbe-276e-4de9-b7d9-9510beb5fd9e",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## INFERENCE\n",
    "\n",
    "# get token ids for texts (= encode)\n",
    "\n",
    "# generate the next terms in the sequence with a random seed\n",
    "\n",
    "# get image from token ids (= decode)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "14ce6527-c9b6-497a-b824-14622d8e85eb",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
