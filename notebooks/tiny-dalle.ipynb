{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 1. Chargement des données d'entraînement\n\nLes images sont issues de COCO, et ont été filtrées pour (tenter de) ne retenir que des images représentant des paysages.",
   "metadata": {
    "cell_id": "40ab0b08-a7bc-4a8a-9736-8df99ecc1f61",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "# !wget https://minio.lab.sspcloud.fr/cthiounn2/archive_val.zip\n# !unzip -o ../image_data/archive_val.zip -d ../image_data/",
   "metadata": {
    "cell_id": "f37a61e6-a659-4b96-a359-f5bda1426c6c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d53b0560",
    "execution_start": 1643448375152,
    "execution_millis": 7951,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--2022-01-29 09:26:15--  https://minio.lab.sspcloud.fr/cthiounn2/archive_val.zip\nResolving minio.lab.sspcloud.fr (minio.lab.sspcloud.fr)... 185.24.184.229, 185.24.184.228\nConnecting to minio.lab.sspcloud.fr (minio.lab.sspcloud.fr)|185.24.184.229|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 85830108 (82M) [binary/octet-stream]\nSaving to: ‘archive_val.zip’\n\narchive_val.zip     100%[===================>]  81.85M  13.9MB/s    in 6.5s    \n\n2022-01-29 09:26:22 (12.6 MB/s) - ‘archive_val.zip’ saved [85830108/85830108]\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "verbose = 0\ntext_token_length = 255\n",
   "metadata": {
    "cell_id": "0d5cc053-1ec9-45ed-a73f-4258868782c6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "39dd11f2",
    "execution_start": 1643464382142,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Entraînement\n\nL'entraînement consiste en:\n\n1. Encoder le texte en tokens-text\n2. Encoder les images en tokens-image\n3. Modéliser l'ensemble de façon auto-régressive\n\n### 1.1. Encodage du texte\n\nNous utilison `BartTokenizer` pour l'encodage du texte comme mini Dall-E. Le Dall-E original utilise selon l'article du \"_BPE-encoding_\" (byte-pair encoding, c'est à dire strictement parlant des paires de caractères), ce qui peut s'interpréter comme l'utilisation du modèle GPT-3, qui repose lui-aussi sur un encodage proche d'un _BPE encoding_. Malheureusement, GPT-3 n'est pas disponible au grand public.",
   "metadata": {
    "cell_id": "c7099527-3f4c-498d-b871-ec1c1fd361fd",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "# TO DO:\n# - save the weights locally to avoid download each time\n# - see if possible to use GPT-3 with Open-AI free account\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nimport torch\n\n# https://huggingface.co/transformers/v2.11.0/model_doc/bart.html\n\ncaption = \"A Emperor penguin standing on the ice\"\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\ntext_tokens = tokenizer(caption, max_length=text_token_length, padding='max_length')['input_ids']\ntext_tokens = torch.as_tensor(text_tokens) # BPE-encoding\nif verbose >= 2:\n    print(text_tokens)\n",
   "metadata": {
    "cell_id": "4960c543-3ac9-4670-a024-15323b7baf1c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "effcf910",
    "execution_start": 1643464428289,
    "execution_millis": 523,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 1.2 Encodage des images\n\nNous utilisons alternativement ",
   "metadata": {
    "cell_id": "a6b11683-82ba-495b-a973-c94d0a228332",
    "tags": [],
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "7dfe3d91-80d9-445c-a633-ca4e528e202f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f60d5470",
    "execution_start": 1643464433128,
    "execution_millis": 7654,
    "deepnote_output_heights": [
     463,
     194
    ],
    "deepnote_cell_type": "code"
   },
   "source": "## TRAINING \nimport io\nimport os, sys\nimport requests\nimport PIL\n\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport torch.nn.functional as F\n\nfrom dall_e          import map_pixels, unmap_pixels, load_model\nfrom IPython.display import display, display_markdown\n\ndev = torch.device('cpu')\n\n\n# get token ids for images (= encode)\n\nencoder = load_model(\"https://cdn.openai.com/dall-e/encoder.pkl\", dev)\n\n\ntarget_image_size = 256\n\n# scale images down to 256x256 (cropping the uneven dimension)\n# we might get problems with some images from the COCO datasets\n# ignore these images as a first approximation\n# or reduce the image resolution ?\n\ndef preprocess(img):\n    s = min(img.size)\n    \n    if s < target_image_size:\n        raise ValueError(f'min dim for image {s} < {target_image_size}')\n        \n    r = target_image_size / s\n    s = (round(r * img.size[1]), round(r * img.size[0]))\n    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n    img = TF.center_crop(img, output_size=2 * [target_image_size])\n    img = torch.unsqueeze(T.ToTensor()(img), 0)\n    return map_pixels(img)\n\n# replace by direct reading from disk\n# persist images to disk in the first place\ndef download_image(url):\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return PIL.Image.open(io.BytesIO(resp.content))\n\nx = preprocess(download_image('https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKIWgaiJUtss/v2/1000x-1.jpg'))\n\nz_logits = encoder(x)\nz = torch.argmax(z_logits, axis=1)\n#z = F.one_hot(z, num_classes=encoder.vocab_size).permute(0, 3, 1, 2).float()\n\n# pad text to fixed length with an additional id and bind text and image tokens together\n\n# model the sequence with a transformer model\n\n",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/transforms/functional.py:405: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\nimage_tokens=z.flatten()\nall_tokens=  torch.cat( (text_tokens,image_tokens) )\n\nif verbose > 2:\n    print(all_tokens.shape)",
   "metadata": {
    "cell_id": "a527766d-88e2-4e72-b628-9e721be47893",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "28ced6d3",
    "execution_start": 1643464448246,
    "execution_millis": 2,
    "deepnote_output_heights": [
     21.1875,
     21.1875
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# QUESTIONS:\n# - Is it a problem that there is potentiel overlap between the indices of image and text-tokens ?\n\nfrom transformers import BartForConditionalGeneration\n\n\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\npredict = model(all_tokens, attention_mask=torch.ones_like(all_tokens))",
   "metadata": {
    "cell_id": "1e027c6c-3e79-4dc3-bb7b-bd75685faa11",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4a952c3",
    "execution_start": 1643464465189,
    "execution_millis": 557,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faa2ccc74cf74db1b0f993c722fee9e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "torch.ones_like(all_tokens)",
   "metadata": {
    "cell_id": "b4bd6a92-d575-4600-9da6-bd86ce8e6a1b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb739972",
    "execution_start": 1643464121273,
    "execution_millis": 3,
    "deepnote_output_heights": [
     21.1875
    ],
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 33,
     "data": {
      "text/plain": "tensor([1, 1, 1,  ..., 1, 1, 1])"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## INFERENCE\n\n# get token ids for texts (= encode)\n\n# generate the next terms in the sequence with a random seed\n\n# get image from token ids (= decode)",
   "metadata": {
    "cell_id": "952dbbbe-276e-4de9-b7d9-9510beb5fd9e",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4f3692ed-5f27-49a4-899a-82a03e72232c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "14ce6527-c9b6-497a-b824-14622d8e85eb",
  "deepnote_execution_queue": []
 }
}