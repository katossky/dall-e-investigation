\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages généraux
%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix} % appendices
\usepackage{hyperref} % liens cliquables
\usepackage{xcolor} % couleur du texte
\usepackage{blindtext} % sections non numérotées
\usepackage[autostyle]{csquotes} % citation en ligne
\usepackage{footnotehyper}
\usepackage{comment} % pour masquer des passages

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliographie
%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
    backend=biber,
    style=apa,
    natbib
  ]{biblatex}
  
\addbibresource{biblio.bib} % import biblio file

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossaire
%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[toc]{glossaries}

\makeglossaries


\newglossaryentry{dalle}
{
  name={DALL$\cdot$E},
  description={The text-to-image model studied in this report},
}
\newglossaryentry{dallemini}
{
  name={DALL$\cdot$E Mini},
  description={The down-scaled open replication of \gls{dalle}, by \citeauthor{wandbdallemini}.}
}
\newglossaryentry{dalletiny}
{
  name={DALL$\cdot$E Tiny},
  description={Our down-scaled replication of \gls{dallemini}.}
}
\newglossaryentry{caption}{
  name={caption, prompt},
  description={The description of an image}
}
\newglossaryentry{token}{
  name=token,
  description={The elementary, discrete (categorical) atom that, together with other tokens, constitutes a text or a (discretely compressed) image. Often a word (or a piece of word) for texts, more difficult to interpret for images. A "token" can also correspond to the vectorial representation of such atom, whenever we succeed in projecting all the tokens into a Euclidean space.}
}
\newglossaryentry{codebook}{
  name={codebook, vocabulary},
  description={The list of authorised tokens values, together with their corresponding vectorial representation.}
}
\newglossaryentry{transformer}{
  name=transformer,
  description={A specific neural-network encoder-decoder architecture popular for texts and images, pioneered by \citetitle{attneed} (\cite{attneed}).}
}
\newglossaryentry{clip}{
  name=CLIP,
  description={An image-text similarity assessor, notably used for re-ranking the images generated from \gls{dalle}.}
}
\newacronym{gpt}{GPT}{Generative Pre-Trained Model}
\newacronym{dvae}{dVAE}{Discrete Variational Auto-Encoder}
\newacronym{vae}{VAE}{Variational Auto-Encoder}
\newacronym{nlp}{NLP}{Natural language processing}
\newacronym{ml}{ML}{Machine-Learning}
\newacronym{mle}{MLE}{Maximum Likelihood Estimation}
\newacronym{elbo}{ELBO}{Evidence Lower-Bound}


% Arthur: I comment all abbreviations that are not used
% \item DL: Deep Learning 
% \item NN: Neural Network 
% \item RNN: Recurrent Neural Network 
% \item dVAE / VQ-VAE: Vector-Quantised Variational Auto-Encoder
% \item NLP: Natural Language Processing
%\item GAN: Generative Adversarial Network
%\item NMT: Natural Machine Translation
%\item LSTM: Long-Short Term Memory
%\item MLE: Maxium Likelihood Estimator
%\item AGI: Artificial general intelligence
%\item GRU: Gated recurrent unit
%\item CNN: Convolutionnal neural network
%\item VQGAN: Vector Quantized Generative Adversarial Network
%\item BERT: Bidirectionnal Encoder Representation from Transformers
%\item BART: Bidirectional and Auto-Regressive Transformers
%\item  OCR: Optical Character Recognition \newline

%\item Embedding: Plongement
%\item Token: Jeton
%\item Backpropagation: Rétropropagation
%\item Codebook: Livre de codes / dictionnaire
%\item Fine-tuning: Apprentissage ad hoc
%\item Caption: Descriptif / Légende
%\item Zero-shot : Sans entraînement spécifique
%\item BPE-encoding : Byte Pair Encoding

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Gestion des marges et mise en page générale
%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[margin=1.2in]{geometry} 
\usepackage{multicol}
\usepackage[inline]{enumitem} % inline enumerate

% add a 4th level (paragraphs)
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}


%%%%%%%%%%%%%%%%%%%%%%%%%
%% Notations, code, mathématiques
%%%%%%%%%%%%%%%%%%%%%%%%%

% \usepackage{listings} % code
\usepackage{listingsutf8}
\usepackage{minted} % code coloré
\usepackage{pythontex}
\usepackage{amsmath} % symboles math
\usepackage{amssymb}
\usepackage{bbm} % dummy sign (fat one)
\usepackage{mathtools} % for equation alginment with \mathrlap
\usepackage{cancel} % for cancelation
\usepackage[scr=boondoxo]{mathalfa} % pour mathscr

%%%%%%%%%%%%%%%%%%%%%%%%%
%% Images
%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\graphicspath{{img/}}

% début relecture Arthur 28 avril 2022

\title{Statistical and empirical analysis of text-to-image generation models}
\author{L. Houairi,
A. Katossky,
A. Philippe,
C. Thiounn }
\date{Mai 2022}

\begin{document}

\begin{titlepage}
\maketitle

Project conducted under the supervision of Benjamin Muller and Matthieu Futeral-Peter at INRIA and Guillaume Lecué at ENSAE.
\end{titlepage}

\renewcommand*\contentsname{Table of Contents}
\tableofcontents

\setlength\parskip{0.2 em} % some space between paragraphs

\pagebreak
\phantomsection % Mandatory line to avoid issues with the hyperlinks in the table of contents
\addcontentsline{toc}{section}{Acknowledgements}
\section*{Acknowledgements} 

\begin{multicols}{2}

We are deeply indebted to our supervisors Matthieu Futeral and Benjamin Müller from INRIA (Paris) for their friendly and insightful management of the project, and for offering us the opportunity to work on natural language processing in the first place. We would also like to thank Guillaume Lecué, our supervisor at ENSAE, for taking the time to proofread our mathematical demonstrations.

We are also incredibly grateful to Boris Dayma for \gls{dallemini}, his on-going effort to transparently reproduce the \gls{dalle} model. He kindly answered to our messages asking for directions, even though we eventually did not have time to pursue the tracks he suggested. We are following his most recent advancement on DALL·E mega, his next generation model, with attention.

\end{multicols}

\pagebreak

\printglossary[title=Glossary and abbreviations]

\clearpage

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{Mathematical notations}
\section*{Mathematical notations}

% definition of shortcuts for the notations
\newcommand{\si}{s_{\mathrm{image}}}
\newcommand{\li}{l_{\mathrm{image}}}
\newcommand{\di}{d_{\mathrm{image}}}
\newcommand{\xn}{\mathbf{x}_n}
\newcommand{\yn}{\mathbf{y}_n}
\newcommand{\llat}{{l_{\mathrm{latent}}}}
\newcommand{\dl}{{d_{\mathrm{latent}}}}
\newcommand{\clat}{{c_{\mathrm{latent}}}}
\newcommand{\lt}{{l_{\mathrm{text}}}}
\newcommand{\ct}{{c_{\mathrm{text}}}}
\newcommand{\dt}{{d_{\mathrm{text}}}}
\newcommand{\fd}{{f_{\mathcal{D}}}}
\newcommand{\pt}{p_\theta}
\newcommand{\pp}{p_\psi}
\newcommand{\ptp}{p_{\theta,\psi}}
\newcommand{\qp}{q_\phi}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\ltppxy}{\mathscr{l}_{\theta,\psi}(x,y)}
\newcommand{\elb}{\underline{\mathscr{l}}}
\newcommand{\elbtppxy}{\underline{\mathscr{l}}_{\theta,\phi,\psi}(x,y)}

$$
\begin{aligned}
n & \text{: the number of examples to learn from} \\
\si & \text{: the image width in pixels} \\
\li \equiv s_{\text{image}}^2 & \text{: the number of pixels} \\
\di \equiv3\times l_{\text{image}} & \text{: the size of the image space} \\
\mathcal{X}\equiv \mathbb{R}^{d_{\text{image}}} & \text{: the image space} \\
x \in \mathcal{X} & \text{: one image} \\
\xn \in \mathcal{X}^n & \text{: the set of available images for training} \\
s_\text{latent} & \text{: the side of an encoded (or compressed) image} \\
\llat\equiv s_\text{latent}^2 & \text{: the number of image tokens (or meta-pixels) in an encoded image} \\
c_\text{latent} & \text{: the number of entries in the image encoding codebook} \\
d_\text{latent} \equiv c_\text{latent} \times l_\text{latent} & \text{: the size of the encoded image space} \\
\mathcal{Z}\equiv\{0,1\}^{d_\text{latent}} & \text{: the image encoding space} \\
z \in \mathcal{Z} & \text{: one encoded image} \\
\lt & \text{: the maximal number of text tokens allowed} \\
\ct & \text{: the number of entries in the text codebook (or dictionary)} \\
\dt \equiv \lt \times \ct & \text{: the size of the text space} \\
\mathcal{Y}\equiv \{0,1\}^{\dt} & \text{: the text space} \\
y \in \mathcal{Y} & \text{: one caption (or prompt, or description...)} \\
\mathbf{y} \in \mathcal{Y}^n & \text{: the captions corresponding to the training images} \\
f(x,y), f(x), f(x \mid z), \text{etc.} & \text{: the true, unobserved (joint, marginal, conditional) density} \\
& \phantom{\text{: }}\text{of the data generating process} \\
& \phantom{\text{: }}\text{with respect to the appropriate probability measure} \\
\fd(x,y), \fd(x), \fd(y) & \text{: the observed (joint, marginal) empirical density}
\end{aligned}
$$

\pagebreak
\section{Abstract}
\textit{Not in present in the examples of reports but this part seems crucial to me}
We could possibly write a version of the abstract in french. 

\pagebreak
\section{Introduction}

\begin{multicols}{2}

As of today, \gls{ml} — this set of statistical and computer science techniques aiming at designing self-improving algorithms — is widely used for a large range of applications, and it is the subject of a lot of ongoing research. In particular, the sub-fields of \gls{nlp} and computer vision focus on the comprehension and generation of human texts and images respectively. Recent models such as \glslink{gpt}{GPT-3} were found to be very good at text generation and attracted a lot of attention. However, recent work at the frontier between the two fields has proved to be the most interesting, since it forces program designers to develop a unified way to describe both images and texts.

Typical tasks for these models are text-to-image (producing an image from a given \glslink{caption}{caption, or prompt}) or image-to-text (giving a caption to an image). Such tasks are interesting for at least two reasons. On the one hand, the ability to generate compelling human language based on images, or quality images based on a prompt, can be viewed as convincing steps towards artificial intelligence, demonstrating a certain degree of comprehension of the world. On the other hand, we can imagine various applications for such models, for example iteratively generating realistic pictures from descriptions in creative industries (architecture, fashion...) or fast image descriptions for visually-impaired persons. 

In the present report, we study one of the few existing text-to-image models, namely the model \gls{dalle} by the company Open AI. The authors of the model train \enquote{a transformer that autoregressively models the text and image tokens as a single stream of data} on a massive dataset of 250 million image-caption pairs harvested from the Internet. One part of the model is a \gls{vae} trained to compress large images into a small representation made of image \glspl{token}, and another is a \gls{transformer} that can model autoregressively the sequence of text tokens from the caption together with the image tokens obtained at the previous step. To put it in another way, given a text description, the transformer can generate a concise sequence of "image tokens" summarizing the image, tokens from which the \gls{vae} component can then recreate a full, realistic picture.

\textcolor{red}{In this report, we describe in detail how \gls{dalle} is supposed to function, and we reproduce a down-scaled version of the model. Generative models being notoriously hard to test since testing involves judging whether some generated images do or do not correspond to a given description (a hard task even for a human), we also explore one idea for automating this process, based on the correct rendering of colours.}

The task is however not straightforward. Indeed, even though \citetitle{zeroshot} (\cite{zeroshot}) was published together with the announcement of the creation of \gls{dalle}, Open-AI decided not to make the model fully public, mainly for ethical concerns. Only the first component (the \gls{vae}) is available. Moreover, the article skims over many useful details. Therefore, in order to understand how this model works, and to generate images for our testing procedure, we have to rely on a public down-scaled replication of the model, \gls{dallemini} (\cite{wandbdallemini})
%\footnote{We also marginally exploit yet another replication of \gls{dalle} called \gls{dalle} Pytorch (\cite{dallepytorch}).}
. At the time of this writing, the same team is working on a promising soon-released \gls{dalle} Mega.

% Ce qui suit est répétitif avec ce qui est dit avant.
% Et est-ce qu'on veut vraiment présenter dans cet ordre?

\textcolor{red}{
During this project, we have three aims:
\begin{enumerate}
    \item To understand the complex architecture of the model
    \item To run a qualitative and quantitative analysis of the images that the model can generate
    \item To replicate a \gls{dalle} version of our own, dubbed \gls{dalletiny}
\end{enumerate}
}

\textcolor{red}{
In the first part, we introduce the theoretical background required to understand \gls{dalle}. Then, we present how we generated images using \gls{dallemini}  and the way we analysed them. In the modelling part, we describe our attempt to train a version of \gls{dalle}.
}

\end{multicols}

\pagebreak

\section{Theoretical and technical background}

\begin{multicols}{2}

In \citetitle{zeroshot} (\cite{zeroshot}), the paper accompanying the release of \gls{dalle}, the theoretical description holds in one paragraph:


\begin{displayquote}
The overall procedure can be viewed as maximising the evidence lower bound (ELB) on the joint likelihood of the model distribution over images $x$, captions $y$, and the tokens $z$ for the encoded RGB image. We model this distribution using the factorisation $p_{\theta, \psi}(x, y, z)=p_{\theta}(x \mid y, z) p_{\psi}(y, z)$, which yields the lower bound

$$
\begin{aligned} \label{eq:elb_theirs}
\ln p_{\theta, \psi}(x, y) \geqslant  \mathbb{E}_{z \sim q_{\phi}(z \mid x)} \Big[\ln p_{\theta}(x \mid y, z)\\
-\beta D_{\mathrm{KL}}\left(q_{\phi}(y, z \mid x), p_{\psi}(y, z)\right)\Big]
\end{aligned}
$$

where:
\begin{enumerate}
    \item $q_{\phi}$ denotes the distribution over the $32 \times 32$ image tokens generated by the dVAE encoder given the RGB image $x^{2}$;
    \item $p_{\theta}$ denotes the distribution over the RGB images generated by the dVAE decoder given the image tokens; 
    \item $p_{\psi}$ denotes the joint distribution over the text and image tokens modeled by the transformer.
\end{enumerate}
\end{displayquote}

The goal of section \ref{derivation} is to put the reader on firm foot for them to fully grasp the number of layers hidden under the authors' concision. Specifically, we will define mathematically our variables of interest, derive a parametric probabilistic model balancing tractability and plausibility and find a sound way to estimate those parameters. Then in section \ref{sec:inference} we will see how one can use the now hopefully transparent probabilistic model to produce image outputs from text inputs, a step known in the \gls{ml} community as "inference". Then in the last section we will discuss the practical implementation of \gls{dalle}. In particular, we will present how \gls{dalle} uses a re-ranking of the generated images using CLIP, another model from Open AI that we will describe. We will also highlight the substantial ways in which \gls{dallemini}, the open replication that we use in the next chapters, differs from \gls{dalle}.

\subsection{Model derivation and estimation} \label{derivation}

\subsubsection{Text-to-image models as (probabilistic) machine-learning models} \label{section:probml}

% debate :
% https://stats.stackexchange.com/questions/408421/is-the-only-difference-between-conditional-generative-models-and-discriminative
% https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm

In the machine-learning field, text-to-image models are a subset of the very general class of \textbf{supervised} methods, that is, methods that try to find the "best" possible map between an input space $\mathcal{Y}$ (in our case: all possible texts) to an output space $\mathcal{X}$ (all possible images) using a given number of pairs $(y,x)\in\mathcal{Y}\times\mathcal{X}$ of examples\footnote{Choosing $y$ to represent an input and $x$ for an output is unfortunate, since the convention usually goes the other way around.
However, we believe that it would bring more confusion to break the notation initiated by \citeauthor{zeroshot} in case one tries to read our report together with the original article.}.
(In the next section, we will precisely define $\mathcal{X}$ and $\mathcal{Y}$, but the following stays valid for a wide range of mathematical spaces.)
What "best" means usually boils down to the choice of \begin{enumerate*}[label=(\roman*)]
\item a parametric family of candidate predictors $\mathcal{F}=\{f_\theta:\mathcal{Y}\to\mathcal{X}|\theta \in \Theta \subseteq \mathbb{R}^F, F\in\mathbb{N}_\star\}$ and 
\item a loss-function $\lambda:\mathcal{X}^2\to\mathbb{R}^+$ measuring the discrepancy, on a given $(x,y)$ example, between the prediction $\hat x = f_\theta(y)$ made from a given input $y$ and the corresponding true input $x$
(with $\lambda$ such that $\forall x \in \mathcal{X}:\lambda(x,x)=0$ and $\forall (x',x)\in\mathcal{X}^2:\lambda(x',x)\geqslant 0$).
\end{enumerate*}
With these definitions and writing $\lambda_i(\theta)=\lambda(x_i,f_\theta(y_i))$ to emphasise the dependency on $\theta$, \textit{training} a model means solving:
\begin{equation} \label{eq:minloss}
    \min_{\theta\in\Theta} \sum_{i=1}^{n} \lambda_i(\theta)
\end{equation}
In practice, we use the gradient of the expression with respect to $\theta$ to guide our search for the optimal $\theta^\star$ and the optimal predictor $f^\star=f_{\theta^\star}$.
Unfortunately, defining such a loss-function $\lambda$ between images is far from straightforward.

Indeed, it is challenging to "supervise" a text-to-image model as it would entail that there exists a meaningful loss function that measures "how close" two images are from each other, and this independently of the caption itself!
First, notice that one unambiguous metric would be a binary function $\lambda(\hat x, x)=\mathbbm{1}(\hat x = x)$,
% However, any (infinitesimal, when it makes sense) deviation from $x$ would be uninformative.
stating that two images are either exactly the same, or completely different, even if they differ only by a single pixel.
That's probably too extreme : in a space as big as $\mathcal{X}$ (see after), it is virtually impossible to get a prediction $\hat x=f_\theta(y)$ that matches $x$ down to the pixel, even with millions of examples.

If this binary measure is not an option, we need a less rigid definition.
Unfortunately, there exists no agreed-upon quantitative distance between images, at least none that makes intuitive sense for a human\footnote{One may for instance define $\lambda$ as the euclidean distance over the values of pixels in the image. But this does not correspond to the proximity between images in any conceivable sense for a human. For instance, an added random noise to the image would vastly augment $\lambda$ whereas a human would hardly see a change. Of course one could answer that the best evaluation would then be one performed by humans.
That is, we could ask a (group of) human assessor(s) to provide quantitative feedback about the discrepancy, in their opinion, between two given images.
But even if we disregard the actual difficulty of the task (can one consistently measure the distance between pairs of images ? even when they do not represent anything concrete like pure random pixels ?) and the potential lack of external validity of such an assessment (we necessarily have to choose \textit{some} humans which have their own biases on what two close images are), the overall process would not suit our need for computing a loss on hundreds of examples at each of the hundreds of steps of the optimisation procedure !}.
Furthermore, the very idea that the distance between two images can be defined independently of their common caption is questionable.
It is on the contrary quite likely that for a more precise caption (say "an Indian child playing in a vast garden" vs. "a boy"), humans would scrutinise images longer until they find more evidence for a difference.
As a consequence, on this specific task, there does not seem to exist an obvious loss function $\lambda$ that could guide a supervised model in the correct direction.

One way to circumvent the problem is to introduce some probabilistic context. In \textbf{probabilistic machine-learning}, we frame the problem more broadly. First, we do not force $\hat x$ the prediction to be deterministic as in $\hat x=f_\theta(y)$. Instead, we add some uncertainty and suppose that we can draw $x$ from some unknown probability distribution $p$, something that we write $\tilde x \sim p(\tilde x|f_\theta(y))$. In turn, we can specify a family of candidate parametric distributions $\mathcal{R}=\{p_\mu(\tilde x, \hat x)|\mu \in \mathbf{M}\subseteq \mathbb{R}^M, M\in\mathbb{N}\}$ modelling the uncertainty around the prediction. For instance, we could define a family $\mathcal{R}=\{p_\sigma:\sigma \in \mathbb{R}_+\}$ so that the uncertainty around a predicted image would $\hat x$ be a normal distribution centred around each individual pixel value $\hat x_i$, i.e. $p_{\sigma}(\tilde x | \hat x)\sim 
\mathcal{N}(\hat x, \sigma^2 \mathbf{I}_{\di})$. We add a tilde in $\tilde x$ to insist on the distributional nature of $\tilde x$ as opposed to the deterministic nature of $\hat x=f_\theta(y)$. More often than not, we write $p_{\mu,\theta}(x,y)\equiv p_\mu(x, f_\theta(y))$. Note that $\mathcal{P}=\{p_{\mu,\theta}:\mu \in\mathbf{M},\theta\in\Theta\}$ is also a family of probability distributions, although a more complex one than $\mathcal{R}$.

With this introduction of a family of distribution functions, we can use \textbf{\gls{mle}} to ground our choice for parameters: the "best" parameter $\mu^\star$ is the one that makes the observed sample more likely. For an arbitrary parametric distribution $q_\mu$ ($\mu \in \mathbf{M} \subseteq \mathbb{R}^Q, Q \in \mathbb{N}_\star$) of a random variable $z$, for which we note $\mathbf{z}_n=(z_1, ..., z_n)$ an observed $n$-sample ($n \in \mathbb{N}_\star$), the maximum-likelihood principle tells us to solve the problem $\max_{\mu \in \mathbf{M}} q_\mu(\mathbf{z}_n)$ (where $q_\mu(\mathbf{z}_n)=\prod_{i=1}^nq_\mu(z_i)$ assuming independence\footnote{\citet{zeroshot} do not make any explicit assumptions about independence between the observations of the sample — they only do so implicitly. And what exactly this independence assumption encompasses is probably up for debate. For instance, the COCO dataset of image-caption pairs we use for our reproduction of \gls{dalle} includes an unusual amount of baseball images. Does the selection mechanism responsible for the very inclusion of images in the database qualify as a dependence mechanism? To what degree is the abundance of baseball images in COCO different from a collection of images extracted from the same movie? In the latter case, the independence assumption would seem rather far-fetched. In the lack of a sound insight on the issue, we follow the rest of the literature in the assumption of independence.}). This is equivalent to $\max_{\mu \in \mathbf{M}} \sum_{i=1}^n \ln q_\mu(z_i)$ — the logarithm being a strictly increasing function, it does not change the maximisation program.

However, in the supervised setting where $z_i=(x_i,y_i)$, we have two competing options to define exactly which distribution correspond to $q_\mu$. We can choose to maximise the conditional $x|y$ or the joint $(x,y)$ likelihood, giving rise to two different strains of models: the first called \textit{discriminative}, \textit{regression} or \textit{predictive} models where  $q_\mu(x_i,y_i)=p_{\mu}(x_i|f_\theta(y_i))\equiv p_{\mu,\theta}(x_i|y_i)$, the second ones \textit{generative} models, where $q_\mu(x_i,y_i)=p_\theta(x_i, f_\theta(y_i))\equiv p_{\mu,\theta}(x_i,y_i)$, without the conditioning. Note that $\forall i=1..n$: $$p_{\mu,\theta}(x_i, y_i)=p_{\mu,\theta}(x_i|y_i) \times p_{\mu,\theta}(y_i)$$ ... and thus that the objective only differ by this crucial $p_{\mu,\theta}(y_i)$ factor.

Predictive models being usually reserved to low-dimensional outputs \citep{probml-advanced}, \cite{zeroshot} opt for a \textbf{generative model} instead. Indeed, we face a situation where the output image space is high-dimensional (see next section). At this point, the reader may be puzzled : isn't the ultimate goal of \gls{dalle} to predict an image \textit{conditional on} a given prompt? The reader is completely right, and we will see later how in the end, because of suitable hypotheses (or restrictions, depending on the perspective) on the family $\mathcal{P}$, we can easily recover $p_{\mu,\theta}(x_i|y_i)$ from $p_{\mu,\theta}(x_i,y_i)$.

To conclude this section, let us rejoice that, after a long detour, we finally succeeded in casting our \textbf{supervised} text-to-image problem as a \textbf{probabilistic generative model}. Given a distribution $p_{\mu,\theta}\in\mathcal{P}$, where $\mu$ controls the probabilistic distribution and $\theta$ controls the deterministic part, we can now define an objective on which we can use classical optimisation procedures to find (or "learn") the "best" distribution :
$$\max_{\substack{\mu\in\mathbf{M}\\\theta \in \Theta}} \sum_{i=1}^n \ln p_{\mu,\theta}(x_i,y_i)$$

Doing so, we have actually just slightly extended the standard (deterministic) machine-learning framework of optimisation to the probabilistic component $p$. Notice the similarity with eq. \ref{eq:minloss} ! It suffices to define $\lambda_i(\mu,\theta)\equiv - \ln p_{\mu,\theta}(x_i,y_i)$ to cast our problem as a standard (deterministic) machine-learning algorithm:
$$\min_{\substack{\mu\in\mathbf{M}\\\theta \in \Theta}} \sum_{i=1}^n \lambda_i(\mu,\theta)$$

In the next section, we will justify the "high-dimensional" nature of $\mathcal{X}$ and $\mathcal{Y}$.

\subsubsection{Structure of the text-image space}

The mathematical nature of a text $y \in \mathcal{Y}$ and of an image $x\in\mathcal{X}$ is a matter of convention. But whatever one's definition, one thing is for sure: the induced space will always be high-dimensional. As a bonus, the reader will get a better sense of what the data used to train the model really looks like.

\paragraph{The (discrete) text space}

In \gls{dalle}, a text $y$ is seen as a sequence of $\lt$ text units (or \emph{text \glspl{token}} or simply \emph{tokens} when the context is evident), where each can take up to $\ct$ different authorised values.
For instance, if text units are ASCII-characters, a text $y$ then consists of 256 character tokens, taking values among the $\ct=2^7=128$ possible character values allowed by the ASCII code
    \footnote{
    Even if ASCII uses 7 bits to encode characters, only 95 of them correspond to actual printable characters.
    The remaining slots are reserved for control characters, of which just a couple remain in use to this day, such as tabulation or carriage return.
    }
.
This \glslink{codebook}{\textbf{codebook}} size $\ct$ is also known as the \textbf{vocabulary size}, especially in the context where text tokens are not individual characters, but sequences of characters such as words or parts of words (subwords).

Since the codebook (or vocabulary, we will use the two interchangeably from now on) lists all the possible values that a text token may take, this allows us to replace any chain of characters with an integer pointing to the entry in the vocabulary. 
In this context, a single text input $y$ is a chain of $\lt$ integer (or discrete, or categorical) variables, each taking value among the $\ct$ possible entries from the code.
We thus consider that $\mathcal{Y}\equiv [\![1,\ct ]\!]^{\lt}$, which is a discrete space of $\ct^\lt$ elements. 

Alternatively, writing $\dt = \lt \times \ct$ and $\mathscr{M}_{a,b}(E)$ the space of matrices of size $(a,b)$ whose elements take values from $E$, we can envision this discrete space as a subset of $\mathcal{Y} \subset \mathscr{M}_{\lt,\ct}(\{0,1\})\equiv \{0,1\}^\dt$, in which the rows sum to one.
This depiction will come in useful when we will want to discuss a probability distribution over the code for each token, in which case we will relax the $\{0,1\}$ restriction and authorise any value inside $[0,1]$ and interpret these values as probabilities.

But how to define the codebook?
There is a trade-off between compactness (for a fixed-size sequence of tokens, a smaller vocabulary entails faster computation) and generalisation (fewer entries mean a given text is rendered as a longer sequence of tokens).
A particularly efficient encoding strategy is when the vocabulary is designed so that frequent sequences of characters occupy only one entry, insuring the most compact encoding possible of the text into integers.
In \gls{dalle} specifically, they use $\ct=16,384$ with an encoding scheme known as \textbf{byte-pair-encoding} (BPE). The strategy is taken from \citet{subword-units}, who describe it as follows:

\blockquote{
BPE is a simple data compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte.
We adapt this algorithm for word segmentation.
Instead of merging frequent pairs of bytes, we merge characters or character sequences.
[...]
We iteratively count all symbol pairs and replace each occurrence of the most frequent pair (‘A’, ‘B’) with a new symbol ‘AB’.
}

The practical vocabulary size (topping at a few tens of thousands) is usually much smaller than the typical vocabulary mastered by an English speaker, which means that a lot of words will be split into subwords units.
Typically, the plural word "rats" will be decomposed as "rat" and "s".
In order to fix the reader's ideas, here is the encoding of the first caption appearing in OpenAI's blog post introducing \gls{dalle}\footnote{OpenAI did not release the tokeniser of \gls{dalle}, but we assume it follows the same principle as their previous models \glslink{gpt}{GPT}-2 and GPT-3, of which only the former has been made public. Both GPT-2 and GPT-3 use a larger $\ct=50257$ vocabulary size (as compared to $16384$ for \gls{dalle}), but the value of the example remains. The code used to obtain the actual encoding is available in appendix \ref{textencoding}}. The phrase :

\texttt{"an illustration of a baby daikon radish in a tutu walking a dog"}

... gets turned into the following encoding, where the \verb+·+ character denotes a new word beginning:

\texttt{an, ·illustration, ·of, ·a, ·baby, ·da, ik, on, ·rad, ish, ·in, ·a, ·tut, u, ·walking, ·a, ·dog}

We see that the short but uncommon word "daikon" is split up into several text tokens whereas the longer word but more common "illustration" is kept as a single one.

\paragraph{The (continuous) image space}

\gls{dalle} considers only square images of fixed side length $\si$, measured in pixels.
Square-sized images exceeding $\si$ are down-scaled whereas non-square images get cropped\footnote{
Cropping the images may have for adverse consequence that some of the image context is lost. It is implicitly assumed that the content of the image remains unchanged after cropping.
}.
Given that each of these $\li=\si^2$ colour pixels is described by three real numbers corresponding to the red, green and blue channels\footnote{
The most used colours-space, RGB-space, maps each colour to the amount of pure red, green and blue light needed for a screen to produce this colour. Traditionally, the values for each of the colour components range from 0 to $255$, but this is only a consequence of the storage convention that a colour should occupy 3 bytes (and 1 byte gives $2^8=256$ integer levels). But on purely abstract grounds, each component continuously covers $[0,255]$, and nothing prevents us from remapping this range to $\mathbb{R}$.
That said, the integer-nature of the colour channels will cause some computational issues that we treat \textcolor{red}{IN SECTION + REFERENCE}.
}, an image $x$ can be considered to belong to $\mathcal{X}\equiv \mathbb{R}^{\di}$ with $\di=3\li^2$.

\subsubsection{Derivation}

So far we have made two points: \begin{enumerate*}[label=(\roman*)]
\item both the text space $\mathcal{Y}$ and the image space $\mathcal{X}$ are high-dimensional ;
\item with high-dimensional examples $(y,x)\in\mathcal{Y}\times\mathcal{X}$, it is better to estimate a generative probabilistic model than a discriminative one.
\end{enumerate*}
That means we need to pick an arbitrary parametric family of distributions $\mathcal{P}$, in which we will try to find the one distribution $p^\star$ that best matches the examples $(\mathbf{x}_n,\mathbf{y}_n)$ in the sense that $\prod_{i=1}^{n} p^\star(x_i,y_i)$ is maximal.

But on which bases should we choose $\mathcal{P}$? There are three main ideas : tractability, expressivity and auto-regressivity. Tractability means that there exists a way to find (or approach) the best candidate $p^\star \in \mathcal{P}$, or more concretely, given that we use numerical (as opposed to exact) methods, that both our optimisation objective function and its derivative can be computed exactly (or estimated accurately). Expressivity means that the model covers a wide range of possible true distribution, and correctly captures what makes texts and images specific. The last idea, auto-regressivity, offers the possibility to recover a conditional (predictive) model from the estimated joint distribution.

In the following, we present the main steps of the construction, and defer to the appendices or to references for more details.

\begin{enumerate}

    \item Because the curse of dimensionality prevents us from estimating $p(x,y)$ directly, we introduce a \textbf{latent variable} $z$, of dimension $\dl \ll \di$, that nevertheless captures the underlying features of the images.
    (See appendix \ref{app:latent} for an in-depth treatment of the interest to include latent variables to cope with the curse of dimensionality.)
    This dimensionality reduction will help tractability, while hopefully preserving expressivity.
    
    Now, using Bayes' theorem twice, re-write: $$p(x,y)=\frac{p(x,y,z)}{p(z|x,y)}=\frac{p(x|y,z)p(y,z)}{p(z|x,y)}$$
    By weak analogy with Bayesian statistics, the authors call $p(x,y)$ the \textbf{evidence} and $p(y,z)$ the \textbf{prior}\footnote{
    This vocabulary comes from the unconditional case where $p(x)=\frac{p(x|z)p(z)}{p(z|x)}$ and where $p(z)$ is here correctly labelled as the arbitrary "prior" over the distribution of the unobserved $z$.
    However, in our case, it does not make much sense to talk about a "prior" over $y$ since we can not "believe" anything about what we actually see for a caption. In our opinion, the "prior" label brings more confusion than it actually helps. For instance, it wrongly suggests that we can "believe" arbitrary things about $p(y,z)$. Such hypotheses can only be made over the distribution of the unobserved $z$.
    }.
    
    \item The nice thing here is that this decomposition stays true irrespective of the distribution of $z$.
    We can thus choose the one we like best!
    Tractability and expressivity will guide this choice. For tractability, we want to be able to compute the derivative of the log-likelihood and we would prefer $z$ to be discrete (since $y$ is discrete and we need to estimate $p(y,z)$ in the numerator).
    
    For expressivity, we would like a flexible class of distribution (that can approach almost any true underlying distribution when estimated) but also that the $z$ space have a meaningful Euclidean distance (in which closeness in the $z$ space would translate to some sense of proximity in the image space for human observers.)
    
    We delay the exact specification of the $z$ distribution to section \ref{sec:dvae} but for now let us highlight the following two \textbf{independence hypotheses} that serve both tractability and expressivity: $x|z \perp\!\!\!\perp y$ and $z|x \perp\!\!\!\perp y$.
    Stated in plain English, these two assumptions embed the natural idea that the distribution of a compressed image $z$ encoding a given image $x$ does \textit{not} depend on the corresponding caption $y$ and conversely that once we know a code $z$, the distribution of the uncompressed image $x$ does not depend on $y$ either.
    This captures the idea that $x$ and $z$ actually represent the same image, and that the passage back and forth between the two representations should be independent of the image's actual caption $y$.
    
    The conditional assumptions bring us to the following decomposition of the $(x,y)$ distribution:
    \begin{align*}
        p(x,y) & =\frac{p(x|y,z)p(y,z)}{p(z|x,y)} \\
        & \overset{\text{hyp.}}{=}\frac{p(x|z)p(y,z)}{p(z|x)}=\frac{p(x|z)}{p(z|x)}p(y,z)
    \end{align*}
    ... where $p(x|z)$ is the distribution of candidate images for a given discrete compression code $z$ ; $p(z|x)$ is the distribution of the candidate sequences of image tokens for a given image ; and $p(y,z)$ is the joint distribution over the text and image tokens.
    
    \item Let $\mathcal{F}\equiv\{f_{\theta,z}|\theta\in\Theta\subseteq\mathbb{R}^{F}, z\in\mathcal{Z}\}$ be the family of parametric distributions approaching $p(x|z)$, the distribution of the images $x$ given latent image features $z$:
    $$f_{\theta,z}(x) \simeq p(x |y,z)$$
    This is in other words a \textbf{stochastic image decoder}, that recreates an image $x$ from a code $z$, ignoring the information contained in the caption $y$.
    
    \item Conversely, let $\mathcal{G} \equiv \{g_{\psi} | \psi\in\Psi \subseteq \mathbb{R}^{G} \}$ be the parametric family of distributions approaching $p(y,z)$, the joint distribution of text and latent image features, and write:
    $$g_{\psi}(y,z)\simeq p(y,z)$$
    By replacing the image $x$ with its encoding $z$, we greatly simplified the modelling problem in comparison with the direct estimation of $p(x,y)$ as (a) $y$ and $z$ are now both discrete and (b) the dimensionality of $z$ is vastly smaller than that of $x$ ($\dl\ll\di$).
    
    Additionally, we make the requirement that $g_\psi(y,z)$ be \textbf{auto-regressive}. Temporarily noting $\mathbf{a}=(y \quad z)$ the concatenation of text and image tokens (where bold font higlights the vector nature of $\mathbf{a}$) and noting $a_k$ the $k$-th element of $\mathbf{a}$, we indeed have the decomposition: \begin{equation*} \begin{split}
    p(\mathbf{a}) = p(a_1) \times p(a_2|a_1) \times p(a_2|a_1,a_2) \times \\ ... \times p(a_{\lt+\llat}|a_1,a_2,...,a_{\lt+\llat-1})\end{split} \end{equation*}
    
    But on the other hand, the explicit fully auto-regressive expression is not tractable in practice because the dependency of the current state $a_k$ on the past states $(a_1,..., a_{k-1})$ is both increasing over time and of considerable size at the last stages. And since each $a_k$ is discrete and can take thousands of possible distinct values, the distribution of $\mathbf{a}$ is in practice impossible to properly evaluate. ($\mathbf{a}$ can take $\ct^\lt\times\clat^\llat$ possible values.)
    
    There are two tricks to resolve the problem. \begin{itemize*}
        \item We can try to summarise all the past variables up to token $k$ into a hidden latent variable $\eta_k$ and
        \item instead of frontally estimating $g_{\psi}$, we can estimate a transition between state $(a_{k-1},\eta_{k-1})$ and state $(a_{k},\eta_{k})$.
    \end{itemize*}
    Let us write $\gamma_{\psi,(a_{k-1},\eta_{k-1})}$ the density over the $k$-th state of the sequence, from state $k-1$.
    
    \item At this point note that we have an estimand $p_{\theta,\psi}$ of the whole distribution $p$ since: \begin{align*}
        p_{\theta,\psi}(x,y,z) & \overset{\text{def.}}{=} f_{\theta,z}(x) \, g_{\psi}(y,z) \\
        & \overset{\phantom{\text{def.}}}{\simeq} \, p(x|z) \, p(y,z) \\
        & \overset{\text{hyp.}}{=} p(x|y,z) \, p(y,z)=p(x,y,z)
    \end{align*}
    Therefore, an approximation for $p(z|x)=p(x,z)/p(x)$ is "known" and could in theory be made explicit. However, the derivation of this quantity involves the unbearable computation of $p(x)$ by integrating $p(x,y,z)$ over both $y$ and $z$.
    
    \item As an alternative, we can bypass this step and just \textit{estimate} $p(z|x)$ with yet a new parametric distribution $h_{\phi,x}\in \mathcal{H}\equiv \{h_{\phi,x}|\phi\in\Phi\subseteq\mathbb{R}^{H},x\in\mathcal{X}\}$, a practice known as \textbf{amortised variational inference}\footnote{"Variational inference" is the idea that $p(z|x)$ could be estimated from an arbitrary family of distributions instead of being derived explicitly from $p(x,z)$. "Amortisation" is the idea that this distribution may depend on $x$.}. Provided that this function family is flexible enough and that we have enough data to estimate its parameters, we should get a close approximation of the true conditional distribution:
    $$h_{\phi,x}(z)\simeq p_{\theta,\psi}(z|x,y)\simeq p(z|x,y)$$
    This is the probability of the image tokens $z$ given the real image $x$, or in other words a \textbf{stochastic image encoder}. The pair $(f_{\theta,z}, h_{\phi,x})$ forms what is known as a \textbf{variational auto-encoder} since with both functions you can generate a code from an image, then generate back the image from the code.
    
    \item It is now time to summarise: so far we have two families of densities, $\mathcal{F}$ and $\mathcal{G}$, that are used to approximate $p(x,y,z)$ (and ultimately $p(x|y)$) and an other one, $\mathcal{H}$, approximating to bypass the complex derivation of an estimand for $p(z|x)$. Using \gls{mle}, we want to maximise (section \ref{section:probml}): \begin{align*}\mathscr{l}_n(\theta,\psi) & \equiv \sum_{i=1}^n \mathscr{l}_{x_i,y_i}(\theta,\psi) \\ & \equiv \sum_{i=1}^n \ln p_{\theta,\psi}(x_i,y_i)\end{align*}
    
    Alas, with the introduction of $h_{\phi,x}$, we can not compute $\mathscr{l}_{x,y}(\theta,\psi)$ exactly. However, noting $\textrm{KL}(\cdot \| \cdot )$ the Kullback-Leibler divergence, we show in appendix \ref{appendix:elbo} that $\forall \phi \in \Phi$, and for a random variable $z\sim h_{\phi,x}$:
    \begin{align} \label{eq:elb_ours}
    \mathscr{l}_{x,y}(\theta,\psi) \geqslant \underbrace{\underline{\mathscr{l}}_{x,y}(\theta,\psi,\phi)}{} \\ \equiv \mathbb{E}_z \Big[\ln f_{\theta,z}(x)\Big] - \textrm{KL}\Big( h_{\phi,x} \| g_\psi(y,\cdot) \Big) \\ =\mathbb{E}_z \Big[\ln f_{\theta,z}(x)+\ln g_\psi(y,z) -\ln h_{\phi,x}(z)\Big]
    \end{align}
    Said differently, there exists a lower-bound on the likelihood known as the \gls{elbo} and there are reason to believe that the closest $h_{\phi,x_i}$ is to $p(\cdot|x)$, the tighter the bound. % (see appendix \label{app:tightness}).
    
    \item After a series of computational tricks giving additional requirements on the $z$ distribution (\ref{appendix:tricks}), we can show that the derivatives of $\underline{\mathscr{l}}_{x,y}(\theta,\psi,\phi)$ with respect to every parameter can be estimated with \textbf{Monte-Carlo simulation}\footnote{In practice, we do not need to make the ELBO-derivatives explicit, and apply \textbf{automatic differentiation} instead.}. This means that we can now use standard optimisation methods such as \textbf{stochastic gradient ascent} to find the maximal $\underline{\mathscr{l}}_{x,y}(\theta,\psi,\phi)$, a hopefully good-enough approximation of $\mathscr{l}_{x,y}(\theta,\psi,\phi)$. (See appendix  \ref{app:estimation} for the details of the stochastic gradient procedure.)
\end{enumerate}

And here we are! At the very core of the estimation procedure lays the \gls{elbo}.

In the very central equation \ref{eq:elb_ours}, $\mathbb{E}_z \Big[\ln f_{\theta,z}(x)\Big]$ can be seen as the \textbf{reconstruction quality} of the image $x$: for a given encoder $h_{\phi,\cdot}$,  we can sample a compressed image $\tilde z\sim h_{\phi,x}$ (where the $\tilde{\phantom{x}}$ emphasises that $z$ is random, as opposed to the observed image $x$) ; for a given decoder $f_{\theta,\cdot}$, we can sample a reconstructed image $\hat x\sim f_{\theta,\tilde z}$. How likely are we to get back to $x$ ?

In turn, $\textrm{KL}\Big( h_{\phi,x} \| g_\psi(y,\cdot) \Big)$ can be seen as a regularisation penalty which is smallest when the following two distributions align: \begin{enumerate*}[label=(\roman*)] \item the distribution of the compressed images $\tilde z$ possibly arising when encoding the true image $x$ and \item the distribution of the compressed images $\tilde z^\prime$ possibly arising after the corresponding caption $y$\end{enumerate*}.

Said differently, our new objective favours on the one hand a good reconstruction of the images and on the other hand an alignment of the $\tilde z$ distribution arising from either the encoding of the image $x$ or the continuation of the caption $y$.

Now, if you compare \citeauthor{zeroshot}'s inequality (equation \ref{eq:elb_theirs}) with ours (equation \ref{eq:elb_ours}), you will find much similarities. In total, there are four divergences:

\begin{itemize}
    \item First, superficial notation differences, since we use $f_{\theta,z}$, $g_\psi$ and $h_{\phi,x}$ where they use $p_\theta(\cdot|z)$, $p_\psi$ and $q_\phi(\cdot|x)$.
    \item Then they make a mistake in the conditioning over $y$, using $p_\theta(y,z|x)$ and $q_\phi(y,z|x)$ where it should have been $p_\theta(x,y|z)$ and $q_\phi(z|x,y)$.
    \item There is also a narrative choice not to make the independence assumptions between $z$ and $y$ clear at this stage, thus writing $p_\theta(x,y|z)$ instead of just $p_\theta(x|z)$).
    \item And finally, the most substantial difference is an extension to (and departure from) the probabilistic interpretation, where \citeauthor{zeroshot} prefer to deepen the view that $\textrm{KL}\Big( h_{\phi,x} \| g_\psi(y,\cdot) \Big)$ is a regularisation term, and thus add a tunable "regularisation weight" $\beta$ to the lot.
\end{itemize}   

This later choice however breaks the probabilistic interpretation of the overall method and is thus not explored in our report.

\subsection{Model inference} \label{sec:inference}

In this section, we suppose that we have trained the model described in the preceding section, that is that we know parameters $\psi^\star$, $\phi^\star$ and $\theta^\star$ such as the \gls{elbo} is maximal over our sample of pairs of texts and images. From there, how do we actually generate images from new, unseen prompts?

The answer involves generating a sequence of image tokens $\tilde z\equiv (\tilde z_1 \; ... \; \tilde z_{\llat})$ based on the tokenised prompt $y$ and then reconstructing $\tilde x$ from $ \tilde z$ (where the $\tilde{\phantom{x}}$ emphasises randomness, as opposed to the given caption $y$). The reconstruction step is easy, as it just requires being able to sample from $f_{\theta^\star, \tilde z}$. But the sequence generation is unfortunately difficult !

For reminder, writing $\mathbf{a}=(y \quad z)$ the concatenation of the text tokens and the image codes, we have not estimated the full distribution $g_{\psi^\star}(\mathbf{a})$ but only the token-to-token transition distribution $\gamma_{\psi^\star,(a,\eta)}$ ! (The bold font is used to highlight that $\mathbf{a}$ is a vector of tokens noted individually $a$, or $a_k$ when needed.) From the $k$-th token $a_k$ and knowing the latent summary variable $\eta_k$, we can easily sample the next token-latent pair $(a_{k+1},\eta_{k+1})$, but clearly not a sequence of $\llat$ tokens! Strictly speaking, eliciting the probability distribution of the $k$-tokens sequences would require to evaluate $\clat^k$ times $\gamma_{\psi^\star,\cdot}$ since each image token can take any of $\clat$ possible codebook values — and this without consideration for the possible stochasticity of the transition!

Concretely, \hypertarget{options-generation}{the actual sampling can be done in various different ways }(\cite{howtogen}):
\begin{itemize}
    \item We can simply take the one token with the highest probability, which is called \textbf{greedy decoding} (or greedy search), but this may lead to unlikely sequences, as the long-term likelihood of \textit{sequences} does not necessarily align with the short-term transition between \text{tokens}.
    \item A more elaborate version of the previous option is called \textbf{beamsearch}. This method involves the choice of a value, the number of beams, that will be denoted $l$ in the following explanation. At step 1, the model chooses the $l$ tokens with the highest conditional probability. Then, for each of these tokens, the model looks at the next $l$ tokens with the highest probability. Based on all those combinations, the model drops all sequences but the $l$ for which the probability of the sequence of tokens is highest (that is, the product of the conditional probability of token at step 1 and step 2). So, at any time, the model bears in mind $l$ possible sequences. This method allows not to miss the high probability token "hidden" behind the low probability token. It is always better than the previous one in terms of the probability of the whole sequence. 
    \item Instead of choosing the highest probable next token, we can also sample from the whole distribution, but this requires restricting the probability distribution in which the sampling is performed, lest we fall back to the initial incalculability. In order to do so, \textbf{top-$k$ sampling} restricts, at each step, the distribution of the possible tokens to the $k$ tokens with the highest probability. The probabilities of the tokens that were filtered are redistributed on the tokens that were kept so that the total probability mass remains equal to one. 
    \item A more adaptive restriction is the one used by \textbf{top-$p$ sampling} (or top-$p$ nucleus sampling). At each step, we keep a certain number of possible tokens so that the probability of all the tokens being kept is greater than or equal to the parameter top-$p$. Top-$p$ and top-$k$ sampling can also be used simultaneously\footnote{In the case of the Huggingface library that we use in our own generation, it is unclear which of the two methods has the priority, but given the order in which things are done in the code, it seems that top-p acts after top-$k$ when both methods are used simultaneously.}.
\end{itemize}


\subsection{Model implementations}

Now that the reader understands the rationale for Dall$\cdot$E's model, they understand that there might be many different flavours of the model depending on the exact values of the parameters and the exact choice of the probability distribution families. Parameters are summarised in table \ref{tab:parameters} and choices of distribution families are described in the following section.

The actual implementation also depends on additional considerations, notably the data collection and training procedures. As part of the final release of Dall$\cdot$E, \citeauthor{zeroshot} are also introducing a completely different model, \gls{clip}, whose task is to assess numerically the proximity between a text and an image. Since we will use CLIP for our own quantitative experiments, it is worth another section.

With variations possible at each and every step, there might be eventually many different flavours of Dall$\cdot$E. At the end of the report, we use \gls{dallemini} a version that has been made public, and highlight the divergence from the original.

\subsubsection{Choice of distribution families}

\paragraph{\gls{dvae}} \label{sec:dvae}

For the variational auto-encoder pair $(f_{\theta,z}, h_{\phi,x})$, \citeauthor{zeroshot} use deterministic neural networks, together with assumption about the distribution of the residuals. The two aspects are not clearly articulated in the paper. Here the description of the two neural networks :

\blockquote{The dVAE encoder and decoder are convolutional ResNets with bottleneck-style resblocks. The models primarily use 3 × 3 convolutions, with 1 × 1 convolutions along skip connections in which the number of feature maps changes between the input and output of a resblock. The first convolution of the encoder is 7 × 7, and the last convolution of the encoder (which produces the 32 × 32 × 8192 output used as the logits for the categorical distributions for the image tokens) is 1 × 1. Both the first and last convolutions of the decoder are 1 × 1. The encoder uses max-pooling [...] to downsample the feature maps, and the decoder uses nearest-neighbor upsampling.}

And here is our understanding of the $z$ and $x$ distributions. On one hand, $h_{\phi,x}$ is set to be the categorical distribution whose weights are governed by the output of the deterministic encoding neural network taking $x$ as input. However, since it is discrete, the reparametrisation trick (see appendix \ref{appendix:reparametrisation}) does not work. The authors use the Gumbel-softmax relaxation, replacing $h_{\phi,x})$ by $h^\tau_{\phi,x})$, a continuous approximation that tends towards $h^\tau_{\phi,x})$ as $\tau \to 0$. On the other hand, $p_{\theta,z}$ is supposed to follow a log-Laplace distribution who weights are given by the decoding neural network taking $z$ as input. (Contrary to many other common choices such as the normal distribution, the log-Laplace distribution ensures that each component $z_k$ of $z$ belongs to the $(0,1)$ interval, and can thus be interpreted as a colour-channel value.)

\paragraph{\Gls{transformer}}

For the auto-regressive component $g_\psi$, \citeauthor{zeroshot} use a family of distributions called "transformers" made famous in the literature by \citetitle{attneed} (\cite{attneed}). We do not wish to enter the details, but a \textbf{transformer} is a sequence-to-sequence neural network, that constantly updates a hidden state variable through a tunable mechanism of \textbf{attention}. (Intuitively, the attention mechanism gives the possibility to learn what exactly is worth remembering from the past of the sequence. For instance, it can learn that whenever it encounters a parenthesis, it must remember a hidden "parenthesis open" state, in order to consistently close the parenthesis later on.)
A considerable advantage of transformers in practice is, because of design tricks, that they are easier than traditional recursive / auto-regressive models to train in parallel on multi-core machines or on clusters.

Contrary to the \gls{dvae}, the transformer has not been released by Open-AI, and isn't described in much detail. It uses a "12-billion parameter sparse transformer" and refer to \cite{long-sequences} for implementation details.
It has \enquote{64 attention layers, each of which uses 62 attention heads with a per-head state size of 64} and \enquote{uses three kinds of sparse attention masks}.

\subsubsection{Training} \label{section:training}

The \gls{vae} is trained first, then the \gls{transformer} is trained separately. The reason for not alternating the training of the two at each step of the optimisation procedure is not clear to us.

It uses "250 million image-text pairs collected from the internet" that :
\blockquote{... incorporates Conceptual Captions, the text-image pairs from Wikipedia, and a filtered subset of YFCC100M. (...) These filters include discarding instances whose captions are too short, are classified as non-English by the Python package \texttt{cld3}, or that consist primarily of boilerplate phrases such as “photographed on <date>” (...) We also discard instances whose images have aspect ratios not in [1/2, 2]. If we were to use very tall or wide images, then the square crops used during training would likely exclude objects mentioned in the caption.}

The original paper devotes considerable treatment for optimising and accelerating the training, down to considerations for the choice between 16-bit or 32-bit float encoding. Meta-parameters such as $\beta_t$, $\tau_t$ and $r_t$ (where $t$ denotes a step in the optimisation procedure) are repeatedly diminished and annealed during training according to specified schedules. (See appendix  \ref{app:estimation} for en example of the estimation procedure with varying $\tau_t$ and $r_t$.)

\subsubsection{\gls{clip}}

When working with image generation as we are, a central issue is the evaluation of the quality of the outputs. How can one decide whether a picture is coherent with a given prompt ? CLIP is one model that can be used for such a task. It provides a number representing the quality of the fit between a prompt and an image and can thus be used as a quality measure of DALLE's outputs \citet{learntransf, openaiclip}. This number can either be a real number, or a probability if, in order to see what picture best matches a text, we normalise the values. We spent some time evaluating if CLIP notation was similar to how humans would do it, this will be described in the Empirical approach part. See appendix \ref{app:more-CLIP} for further details on CLIP. 

\subsubsection{Variations: \gls{dallemini}, \gls{dalletiny}}

We used 3 datasets for our model:
﻿Conceptual Captions Dataset which contains 3 million image and caption pairs.
﻿Conceptual 12M which contains 12 million image and caption pairs.
The OpenAI subset of YFCC100M contains about 15 million images and we further sub-sampled to 2 million images due to limitations in storage space. We used both title and description as caption and removed html tags, new lines and extra spaces.




\end{multicols}

\begin{table*}[h!]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
        ~ & \gls{dalle} & \gls{dallemini} & \gls{dalletiny} \\ \hline
        Tokenizer & GPT-3 (probably) & Bart & Bart \\ \hline
        Autoencoder & dVAE & VQ-GAN & VQ-GAN \\ \hline
        Autoregressive component & Sparse transformer & Bart (fine-tuned) & Bart (fine-tuned) \\ \hline \hline
        Used data set & Custom & Custom & MS COCO 2017 \\ \hline
        Training set size & > 250M & ~ & 12,134 \\ \hline \hline
        Caption max length (tokens) $\lt$ & 256 & 255  & 255 \\ \hline
        Resolution of original images (pixels) $\li$ & 256×256 & 256×256  & 256×256 \\ \hline
        Res. of compressed im. (tokens) $\llat$ & 32×32 & 16×16 & 16×16 \\ \hline
        Image codebook size $\clat$ & 8,192 & 16,384  & 16,384 \\ \hline
        Text codebook size $\ct$ & 16,384 & 50,265 & 50,265 \\ \hline
    \end{tabular}
    \caption{Parameters and distribution functions for different versions of \gls{dalle}}
    \label{tab:parameters}
\end{table*}

\section{Data}

\begin{multicols}{2}
The dataset used by Dall$\cdot$E was not released publicly. In this section, we introduce two alternative data sets of image-caption pairs that we use for evaluating the performance of \gls{dallemini} and for training our own \gls{dalletiny} implementation: \textbf{MS-COCO} and \textbf{Flickr30k}.

\subsection{MS-COCO}

\subsubsection{Presentation}

The MSCOCO dataset is a well-known dataset of captioned images which is available at \href{https://cocodataset.org/}{MSCOCO's website}. (Alongside the caption, it also includes numerous other details such as object names and location, which is used for object detection.) There are two main versions of the dataset from 2014 and 2017 respectively. The 2017 MSCOCO dataset is split into training, testing and validation sets of images and training and validation annotations. The size of the training dataset is $118,287$ and that of the validation dataset is $5,000$.

\subsubsection{Use}

We access the MSCOCO dataset via the Python library \texttt{fifty-one}. 
We first use the 2017 MSCOCO dataset for evaluating \gls{dallemini}.
We extract a sample of 100 images and pick one caption out of 5 and feed them to \gls{dallemini} to generate 10 images.
Then, we analyze the generated images with the base image, with CLIP and manual evaluation.
We also generate 2 other captions for each base image.

We also used 3 prompts and the corresponding images from this dataset for the analysis of the influence of the generation parameters on the quality of the images.

Finally, we use the 2017 MSCOCO dataset for training our model, \gls{dalletiny}. We first extract landscape-related captions and extract the sub-dataset and feed the latter to our model.
As a second experiment, we also use the whole training dataset for our model.

\subsection{Flickr30k}

\subsubsection{Presentation}

Flickr30k is another well-known dataset of images from Flickr, containing around 31 000 images.
We use the Flickr30K Entities Dataset from \cite{flickrentitiesijcv}

\subsubsection{Use}

We use the Flickr30k dataset for evaluating dalle-mini. Unlike the 2017 MSCOCO dataset, we generate 5 images for each caption of the testing dataset.
We compare the given CLIP score of each caption-image pair and run a color analysis on the generated images.

\end{multicols}

\section{Empirical approach}

\begin{multicols}{2}

The capabilities of text-to-image may be impressive at first sight, but as stated in \cite{probml-advanced}, they are notoriously difficult to evaluate objectively. In this section, we present our attempts to develop such a rigorous automated assessment. Since \gls{dalle} is not public, we rely on \gls{dallemini} instead. We also heavily rely on \gls{clip} for our automation effort, and that is why we start with an exploration of CLIP's capabilities.

% The main objective of this empirical study is to better understand the models studied, in order to integrate these lessons into our own modelling attempts.

%\begin{itemize}
%    \item \gls{dallemini} differs from its big brother in architecture and capacity, we want to test these generation capabilities in both qualitative and quantitative ways. We want to be able to present the strengths of the model and give the parameters to use according to the desired generations
%    \item CLIP is an integral part of this process since it assigns a score to the images that can be used to draw conclusions. We want to better understand the scores assigned by comparing them to a human judgement and determine if we trust CLIP to judge \gls{dallemini} images.
%\end{itemize}

\subsection{CLIP}

The reader might remember from earlier sections that CLIP is a model which takes an image-caption pair as input and computes an adequacy score between them. The (absolute) scores range on an unbounded scale where higher scores mean higher adequacy. When dealing with several images, absolute scores can then be rescaled to (relative) probability scores. But how reliable are these scores? We will test CLIP adequacy with human judgement, but also get a more quantitative look by seeing what CLIP produces when scoring datasets of different qualities and assess its sensitivity to small changes in image quality. \textcolor{red}

\subsubsection{CLIP and humans agree on text-image adequacy}

% CLIP is able to generate a matching score between a caption and an image in absolute value (this image is worth $x$ points) or a probability score between a caption and several images, a relative score. CLIP is also able to work in reverse, proposing a probability score between an image and several captions: it gives the caption that matches the image the most according to it.(See figure ~\ref{fig:clip_proba_exemple}).

% \textit{Si CLIP scoring already explained:} As mentioned before, CLIP is able to evaluate images and captions in different ways.

To test our confidence in CLIP, the following experiment is performed to determine if CLIP and humans tend to make the same choice when presented with different captions: 
\begin{itemize}
  \item Selection of 100 random images from the COCO dataset.
  \item Writing of 3 descriptive captions: one caption from the dataset, the other two chosen by us (from very adequate to not at all).
  \item Notation of the correspondence between the image and the sentences chosen by each human.
  \item Comparison with the adequacy score established by CLIP.
\end{itemize}

The results can be found in the appendices, figure \ref{fig:Humans vs CLIP}, they show that on most caption/image pairing, CLIP and humans agree on the score to give. All good captions for CLIP are considered good by humans. However, the limitation of the experiment is that some captions are so accurate that the probability score is very close to 1 for CLIP, causing other captions that are relatively good from a human perspective to score close to 0, which gives a messy column for this specific score.

\subsubsection{Inquiries on CLIP using Flickr's test set}

Now that we are confident in CLIP's ability to provide meaningful scores (a qualitative test), we test quantitatively the quality of the images generated by \gls{dallemini} using the model scores. 

Using the whole Flickr test set, for each prompt: 
\begin{itemize}
    \item We have the original image from the data set and CLIP's score between the two, this is the first list of scores.
    \item We generate five different images for convenience, then we take the mean of CLIP's scores of the five images, sample randomly one of the five images generated, and keep its score, this is the second list of scores. For the computation of the correlation coefficients, we take the mean of the five scores instead of sampling one score randomly among the five. 
\end{itemize}

We obtain:
\begin{itemize}
    \item For the gold images (the original images): a mean score of 32.4 with a standard deviation of 3.4.
    \item For the generated images: a mean score of 23.3 with a standard deviation of 3.5.
\end{itemize}

The value of these scores (in absolute value) does not represent anything, but shows the magnitude of CLIP's criticism of the generated images.

Using these scores, it seems that CLIP behaves like a good classifier of generated image vs real image. If presented with a caption and two images (one real, one generated), CLIP can determine which one is real and which one is generated with an accuracy of 96.3\% only by comparing the two images scores.

Now is that a success of CLIP or a failure of \gls{dallemini}?

In an attempt to characterise the quality of the images generated, we compute two correlation coefficients between the two list of CLIP scores, namely Spearman's and Pearson's correlation coefficients. Both yield a small but significant correlation coefficient (at the 5\% level):
\begin{itemize}
    \item $\rho$ = 0.09, p-valeur = 0.39\% for Spearman
    \item r = 0.113, p-valeur = 0.03\% for Pearson
\end{itemize}
Both correlations are close to 10\%. If CLIP is working correctly and the image generation model is good as well, the images generated should be of good quality, and thus their adequacy score with the prompt should be of the same order as those of the original image. Therefore, we expect higher correlation coefficients. 

A possible conclusion is that two images can fit well to a prompt while having different CLIP scores, and thus that CLIP is not adapted for fine comparisons of images, and thus neither for fine comparisons of the image generating models. 

\subsection{Assessment of \gls{dallemini}}

Now that we have a (relatively) valid automated judgement mechanism in the form of the CLIP model, we can turn to our core task, namely assessing the capabilities of Dall$\cdot$E sister model, \gls{dallemini}. We start with qualitative, human-made assessments, then we explore the practical effect of generative parameters (the theory of which has been presented in section \ref{sec:inference}), before turning to the suggestion of an automated colour-rendering adequacy test.

\subsubsection{Qualitative exploration of generation capabilities}\label{subsubsection:qualitativegeneration}

If the generations of \gls{dalle} that can be found on the Open-AI website suggest an exceptional performance, the possibilities of \gls{dallemini} are more limited. In this section, we perform a series of crafted tests (as opposed to automated ones) to explore the model's capabilities. The objectives of this first qualitative observations are to:

\begin{itemize}
    \item Gain a good overview of the model by testing a number of features to understand these strengths and weaknesses.
    \item From this overview, adapt the model parameters to try to refine the prediction results. 
    \item Find points of approach for a more quantitative analysis of the model, exploitable on \gls{dallemini} as well as on other generation models to be able to compare them.
\end{itemize}

We generate a multitude of images based on crafted captions. We start with very basic prompts listed in table \ref{tab:prompts}. Examples of such generations can be found in figure \ref{fig:first_generations}.

\begin{table*}[]
    \centering
    \begin{tabular}{|l|c|r|}
        \hline
        Feature & example & caption example \tabularnewline
        \hline
        Basic geometry & circle,ball,triangle & \texttt{a picture of a cube} \tabularnewline \hline
        Common items & bed, cake, heart & \texttt{trees in a circle} \tabularnewline \hline
        Shapes and colors & blue, red, ball & \texttt{a green and yellow ball} \tabularnewline \hline
        Texture & wood, steel, grass & \texttt{a building made of wood}\tabularnewline \hline 
        Quantifier and size & big, small, A, some & \texttt{a big tree} vs. \texttt{some trees} \tabularnewline \hline
        Syntax & A, a picture of, a photo of & \texttt{a tree} vs. \texttt{a photo of a tree} \tabularnewline
        \hline
    \end{tabular}
    \caption{Prompts construction for testing basic features of image generation}
    \label{tab:prompts}
\end{table*}

The results are ambivalent, most of the images proposed in the appendix give a glimpse of the (partial) success of the model. The layout of some simple shapes or objects (a cube, a table) and the poor management of quantifiers are disappointing.
However, we are pleasantly surprised by the management of colours and textures, as \gls{dallemini} almost always represents faithfully the requested ones.
Seemingly innocuous caption changes such as starting your caption with "an image of" is likely to raise the quality of the image produced.
(This syntactic construction is over-represented in the \gls{dallemini} training dataset, which explains why some apparently longer or more complex captions finally give better results.)

This significant influence of the training dataset makes us reconsider the possibilities of model generation. Since landscapes are very present in the training dataset, we are confident that we would get better results by trying to draw them.
Thus we shift focus from simple shapes to more complex topics based on what exists in the training set.
So we launch a new generation series from more complex texts with the following prerequisites:
\begin{itemize}
    \item Important use of 'a picture of' or 'a photo of'.
    \item Generation of particular landscapes or features that are prominent in the dataset (baseball games for unknown reasons).
    \item Building captions containing several instructions to evaluate the performance of \gls{dalle} to treat every information required.
\end{itemize}

Some of the results are shown in figure \ref{fig:complex_generations}

It is with pleasure that we discover the ease with which \gls{dallemini} draws landscapes. After the relative failure of drawing simple shapes and respecting quantifiers, we are also pleasantly surprised by the management of positional requirements (behind, in) and the ability to display several of the requested elements. Based on these observations, we want to focus part of our analysis on the colour management of \gls{dallemini}, which sometimes seems to be remarkable and sometimes overlooked. A difference that is not always correctly evaluated by CLIP. Indeed, an image that does not show a requested colour sometimes gets a better score than images that respect the colour indication.

\subsubsection{Study of the influence of generation's parameters}

So far, we have not spoken of the different options of generations we presented \hyperlink{options-generation}{in the theoretical background part}. We want to know if modifying them, as compared to the default set of parameters, can affect the quality of the generated images.

The standard parameters in the implementation we use are:
\begin{itemize}
    \item \textit{do\_sample = True} so we are sampling in the distribution and not using greedy decoding.
    \item \textit{top-k = 50}.
    \item \textit{top-p = 1.0}.
\end{itemize}

We experiment on two lines of research:
\begin{itemize}
    \item Setting \textbf{do\_sample = False} in order to use greedy decoding, using different number of beams.
    \item Vary the values taken by the parameters top-p and top-k independently.
\end{itemize}

The first experiment did not yield interesting results. For different prompts, the output is either totally black, totally white or very strange. We did not find any explanation for this strange behaviour. However, we note that the score of the adequacy of a prompt with a white image as given by CLIP could reach up to 21 or 22. Thus, it seems that CLIP does not yield a value of 0 when we, as humans, would consider it appropriate. See appendix \ref{app:greedy-images} for examples of images obtained using greedy decoding. 

For the second experiment, we select 3 images from the MS-COCO data set we used earlier, for which we have a prompt and a CLIP score linking the two. We also create manually a prompt for a landscape, as we know the model we use is good at generating such images. The prompts we used are
\begin{itemize}
    \item "The man with pierced ears is wearing glasses and an orange hat."
    \item "A black and white dog is running in a grassy garden surrounded by a white fence."
    \item "A young female student performing a downward kick to break a board held by her Karate instructor."
    \item "A sunset over snow-capped mountains" (synthetic prompt).
\end{itemize}
We also add "A picture of" at the beginning of each prompt, for we knew it allows us to produce slightly better images. The scores of adequacy between the original picture and the new prompt (i.e. "A picture of the man ...") were recalculated, and were, interestingly, slightly different (as compared to the scores of the image with "The man ..."). 

We test 9 sets of parameters. The first is our reference: top-k = 50, top-p = 1.0. We create eight alternative sets by varying one of the two parameters, one at a time. We test the following variations: top-k = [5, 25, 100, 200] and top-p = [0.1, 0.3, 0.7, 0.9]. For each of the 9 sets of parameters and for of the each four prompts, we generate 20 images, then score them against their prompt using CLIP. Considering the huge number of generated images, we solely rely on CLIP to analyse the images generated. (Recall that this score is supposed to quantify the adequacy of the original image with its text description.)

Then, we compare the scores quantitatively using two statistical tests for each caption and set of parameters:
\begin{itemize}
    \item \textbf{Comparison with the CLIP score of the original image (gold score)}. A Student test is performed to assess whether the average CLIP score of the 20 generated images is different from the CLIP score of the original image.
    \item \textbf{Comparison with the standard set of parameters}. A Welch test (Student test not assuming equality of variance) is used to assess whether the average CLIP score of the 20 generated images is different from the same average, but over 20 images generated from the standard set of parameters.
\end{itemize}

The following figure presents our results for the caption "A
picture of the man with pierced ears is wearing
glasses and an orange hat.” (see appendix  \ref{app:other-gen-param} for the figures corresponding to the 3 other prompts). The x-axis refers to the different sets of parameters used for generating the images. The grey line indicates the CLIP score of the original image. The black dots are indicating the average CLIP score of the 20 generated images, for each set of parameters. The confidence bounds at 95\% were obtained using the quantiles of the Student distribution with 19 degrees of freedom. 
Finally, the stars and dots at the top of the picture indicate the result of the different tests. (The black symbols refer to the Student test and the red symbols to the Welch test. One dot indicates significance at 20\%, two dots significance at 10\%, one star at 5\%, two stars at 1\%.)

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{man-ears.png} % scale = 1.0
    \caption{Parameters variation for the prompt "A picture of the man with pierced ears is wearing glasses and an orange hat."}
    \label{fig:gen-man-ears}
\end{figure}

From this image and the three available in the appendices, we clearly see that the scores of the image generated are below the score of the original image. On another hand, the different sets of parameters are not leading to radically different CLIP's scores, as none of them are significant at the 5\% level. Therefore, we don't have a clear incentive not to use the standard set of parameters.  

\textcolor{red}{PARLER DU PROBLEME DE NON GAUSSIANITE}

We conclude that varying the value of top-p and top-k has a low impact (the alternative hypothesis being that CLIP is altogether not suitable for such a task.)

\subsubsection{Colour analysis}

Generative is notably hard to evaluate, the purpose of this section is thus to focus on a specific aspect of the generation, namely the colours.
Indeed, our qualitative assessment tends to show \gls{dallemini} can be pretty good at taking into account colour requests, and automating colour detection and matching was feasible.
We describe a tool to judge the quality of the generations, that can be used to assess the capabilities of text-to-image regarding colours.

But first let us check whether \gls{dallemini} renders colours in a similar way as what can be found in naturally occurring images.  Is the generated colour distribution similar to what can be found in a dataset, or is the generation biased?

To test this, we build two groups of images: a set of images from the Flickr dataset and a set of images generated by \gls{dallemini} using the same Flickr captions. We compute the colour histograms for each group of images and then plot the difference in histograms colour by colour (RGB) (see figure \ref{fig:histograms}). We visually observe conformity between the two distributions (except for the pixels at the ends). Is this adequacy confirmed by a goodness-of-fit test?

After performing a Kolmogorov–Smirnov test on each sample, the null hypothesis (being that the two distributions are the same) is rejected well below the usual thresholds for the red and blue colours. Only the green distribution is rejected at the 3\% threshold. This allows us to conclude that even though the colour distribution is not that different, a slight bias still remains in \gls{dallemini} generations.

We are now ready to introduce our colour recognition tool adapted to our needs to try to understand how good \gls{dallemini} was at drawing the colours requested in the captions. We look at different situations to evaluate the performance of \gls{dallemini}. %  to then compare them to our own model % we do not do that at the end

Using the $K$-means algorithm, we are able to recover the $K$ main colours of each image (see figure \ref{fig:kmean}). We are then able to filter images to recover only those with the desired colour. All this is possible thanks to the CIELAB color space, which defines a metric between colours for which distance mimics image distance perception by the human eye. When "blue" is requested, the images are converted from RGB to CIELAB. Then a threshold can be chosen, and all the colors a distance less than the threshold are returned.

To calibrate this image detection algorithm, 2 parameters are crucial: the threshold, which represents the tolerance between the required colour and the colour observed on the image, as well as the number $K$ of average colours evaluated for each image by the algorithm through the $K$-means algorithm. A high $K$ means more sensibility to small patches of colour, at the cost to return anecdotal colours. This calibration is performed by applying the algorithm to image-caption pairs in the Flickr dataset that includes colour information in the caption, until each colour mentioned in the caption is actually detected by the algorithm.

With this tool available, we generate many images by requesting certain colours in particular, for instance : "a red mountain in the sun". For each colour, we generate 25 images, which we then filter to verify the presence of the requested colour and get an accuracy score. (An example of such research in our notebook can be found in figure \ref{fig:nb_color}.)

We perform a control generation by asking \gls{dallemini} to generate the same captions without asking for a specific colour ("a mountain in the sun"), and compare it to the images where an explicit color was requested. Indeed, there is no reason why a non-requested colour would not appear in a randomly generated image.

We obtain the following dataframe:

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{df_color.png}
    \caption{The results of our color analysis generation global presence represents the control group with no colors asked}
    \label{fig:df_color}
\end{figure}

The results are ambivalent, as they are very dependent on the requested color in multiple ways: 

\begin{itemize}
    \item Some colors are present by default in any given image (white, black and grey given the permissive threshold).
    \item The global presence score must be treated carefully, as the images requested may influence it even though no color is explicitly required (some captions mentioned the sea, therefore the blue in very present on its own in the previous results table).
    \item Some colors seem to be more represented in the training dataset and thus are more likely to be found in any given image. (When asking for the image of a ball, even though it apparently suggests no color, in addition to white and black, green and yellow over overrepresented. This could be explained by the presence of many green landscapes in the dataset).
    \item Apart from the two important parameters for calibrating the color detection tool, it is also necessary to choose the requested color (in RGB) with care. If we ask for a color that is too bright, the conversion to LAB may produce detection results different from what a human would have identified. It is therefore necessary to choose carefully the coding of the requested colors.
\end{itemize}  

Although with significant variability depending on the choice of color (in rgb coding), we conclude that \gls{dallemini} is able to understand the color instructions, but fails to be consistent. Indeed if the presence percentage improves, a qualitative look at the whole images generated shows that either \gls{dallemini} draws it or it does not care at all. However, using this tool to filter multiple generations of the same caption is just a good way to improve the model generation, this tool could complement CLIP to score and rank the images generated from a single caption to find the best to show.

\end{multicols}

\section{Modelling}

\begin{multicols}{2}

In this section, we build our own implementation of \gls{dalle}, dubbed \gls{dalletiny}. Code is located on \href{https://github.com/cthiounn/dalle-tiny}{Github}. In order to reproduce DALLE's functionalities, the first step before implementing is to link the initial paper to the different already available implementations, most notably Boris Dayma's \gls{dallemini} (and more anecdotally Lucidrains' Dall$\cdot$ Pytorch). Then, we pick an image-caption dataset, a \gls{vae} and an autoregressive distribution family. Finally, we train and test our model.

\subsection{Methodology}

For each experimentation, we follow the same methodology in ten steps :

\begin{enumerate}
\item Pick a dataset and split it into train and test sets
    \item Opt for a dVAE model for compressing images into a compressed sequence of image tokens and vice versa
    \item Choose a text tokenizer for translating the text into a sequence of text tokens
    \item Preprocess data with the dVAE's encoder for each image in the dataset
    \item Preprocess data with the text tokenizer for each caption in the dataset
    \item Select a seq2seq/transformer model and adapt its decoder (indeed, with both a small dataset and relatively limited computing power, we are not able to train a transformer with billions of parameters from scratch ; instead, we prefer to "fine-tune" preset existing transformer models)
    \item Implement a training loop
    \item Perform a manual hyperparameters search
    \item Launch with a chosen set of hyperparameters and save the model at a regular time-frequency
    \item Implement an inference code using a caption, the model and the dVAE's decoder
\end{enumerate}

\subsection{Tinydalle implementation on landscapes of 2017 MSCOCO dataset}

\subsubsection{Dataset selection}

Since in section \ref{subsubsection:qualitativegeneration}, \gls{dallemini} showed good results for landscapes, we decide to focus on landscapes examples. We start with the entire 2017 MSCOCO dataset, filter specific keywords of landscape, such as "mountain", "sea" in all captions, and exclude keywords of individuals, such as human and animal related-keywords. Since in the dataset, each image has a set of five captions, we pick the first of the five. Finally, we extract a landscape-related dataset of 12,134 train images and 506 test images from a starting size of respectively 118,287 and 5,000.

\subsubsection{dVAE model selection}

For dVAE model, we opt for the VQ-GAN model trained by Boris Dayma and used in \gls{dallemini}, which compresses an image into a sequence of image tokens with a vocabulary of 16384 image-tokens. An image of 256*256*3 (W*H*C) is converted into a sequence of 16*16 image tokens.

\subsubsection{Text tokenizer selection}
Like \gls{dallemini}, we start from Huggingface's implementation of BartModel, so we use the BartTokenizer to process captions. It can translate a caption into a sequence of text tokens with a vocabulary of 50265 text tokens. We pad the output sequence to 255 text tokens.

\subsubsection{Data preprocessing}

Before the training phase, we preprocess data so that an image-caption pair is turned into a pair of a sequence of text tokens and (compressed) image tokens. Since the operation is the same for each step of the training phase, we choose to pre-process the texts and images beforehand to speed up the training process subsequently. Similarly to \gls{dalle}, we resize and center-crop\footnote{the center-cropping operation may induce loss of information and can mismatch the cropped image with the caption if there are items at the border} the images to fit into a 256*256 pixel box.

\subsubsection{Auto-regressive modelling}

We begin from the last checkpoint of BartModel and we reset its decoder in order to adapt the output of the Seq2Seq/transformer. BartModel usually ingests text tokens and outputs text tokens. Therefore, the decoder part must be transformed to output image tokens. To do so, we change the decoder configuration and the layer of the inner neural network in order to match the size of the image vocabulary. Since modelling image tokens is a new task for BartModel, we also reset the weights of all layers of the decoder.

Furthermore, BartModel comes pretrained (with learned parameters) incorporating general knowledge of natural language processing that would be unfortunate to lose. So we decide to freeze the encoder part (that should embed an "understanding" of the English language, to some compact encoding) while fine-tuning the decoder weights. This is intended to improve learning abilities on a relatively small dataset like ours, since there are fewer parameters to modify.

\subsubsection{Training loop design, hyperparameters search and training phase}

Next, we fine-tune our seq2seq model with the training dataset and evaluate it with the testing dataset, through many steps and epochs. (One step is one optimisation step, only limited by the memory capacity of the computers we work on, usually a few images at a time. One epoch is a complete loop over the training set.) The training loop operates with data parallelism and can use multiple GPUs to compute the prediction of the model. For this computation, the model uses the $256$ image tokens as the target and the 255 text tokens and the $256-1=255$ image tokens as the input of the prediction.
Afterwards, both losses and back-propagation corrections are computed. In order to do so, the loss function is the cross-entropy function which computes the "distance" between a target sequence and a predicted sequence. Since there are many ways to back-propagate, a strategy with the optimiser AdamW with a decay of the learning rate has been implemented.
Deep learning metrics, such as the mean loss on the testing dataset and the mean loss on a small batch of training dataset are frequently sent to \href{https://wandb.ai/cthiounn/dalle-tiny}{WandB} in order to track different runs and to do a hyper-parameters search.
We perform a manual hyper-parameters search for choosing the most favourable learning rate and eventually pick a rate around $5\times 10^5$ (with AdamW).

We use \href{https://datalab.sspcloud.fr/}{Onyxia datalab}'s resources to perform the training. The datalab offers fastAI instances with S3 storage and 1-3 GPUs per instance. Most of the runs are using at least 2 GPUs to speed up the training and to fit a larger batch size into the GPUs. An accumulated gradient strategy has been made steadily on the last runs.

To avoid overfitting the training dataset, we stop the training when the mean loss on the testing dataset has reached its lowest point and save the model weights and configuration.

\subsubsection{Inference phase}

We load the adequately trained-model checkpoint and feed a caption into the text tokeniser and then into the model. The trained model generates a new sequence of image tokens, according to parameters such as top-k, top-p (see section \ref{sec:inference}). Subsequently, the sequence of image tokens is sent into the dVAE's decoder and is translated into a new image.

\subsection{Other experimentations}

\subsubsection{Tinydalle implementation on all 2017 MSCOCO dataset}

In this second experimentation, we use the entire 2017 MSCOCO dataset instead of extracting landscapes. Furthermore, we decide to keep all the captions, since a picture can be described in many ways in the natural language. Thus, the training dataset's and the testing dataset's sizes are respectively $118,287\times 5$ and $5,000 \times 5$.
Since only the dataset selection has changed, the other parts remain unchanged. We keep our implementation but with a bigger and more generic dataset.

\subsubsection{Accumulated gradient strategy}

An accumulated gradient strategy is used to avoid correcting the gradient too heavily with respect to the considered current batch when the batch size is small, resulting in unnecessary back and forth adjustment. Instead of back-propagating at each batch treated, we thus accumulate gradient computation and then back-propagate every chosen number of steps and reset all gradient computations subsequently. Because of the lesser number of errors, the accumulated gradient strategy should result in quicker learning.

\subsubsection{To freeze or not to freeze the encoder ?}

\citeauthor{Thompson_2018}'s article suggests that freezing a sub-network of the model has no significant effects on its performance with continued training. \cite{https://doi.org/10.48550/arxiv.2103.05247} show that fine-tuning only the output layer results in overfitting and degrading significantly the performance. Also, freezing self-attention and feed-forward layers do not degrade the model performance.
The upside of freezing a sub-network of a model is to compute fewer gradients, and to fit more data into the GPUs, resulting in faster computation and training.

\subsection{Tinydalle's results}\label{subsec:tinydalle_results}

\subsubsection{Main results}

We fail to reproduce \gls{dalle}'s functionalities with both the landscape dataset and the complete 2017 MSCOCO dataset. We do succeed in generating new images. However, they are not related to the proposed caption. In the training phase, average test losses initially decrease during a short period of training, then start to increase. That increase means that the model is overfitting the training dataset. The model with the lesser average test loss generates color-uniform images, whereas the model with a later checkpoint generates a well-composed image. However, they are unrelated to the submitted caption.

\subsubsection{Other results}

The accumulated gradient strategy results in a smoother learning curve as expected but decreases both learning time and overfitting, resulting in a steady plateau for both average test loss and training test loss for a higher number of steps of accumulating gradient.

With a small dataset, an overfitted model generates new images in the training dataset token distribution, with no regard for the submitted caption.

Freezing the encoder part results in faster overall learning but also in reaching the overfitting phase quicker

\end{multicols}


\pagebreak

\section{Conclusion}

\pagebreak
\phantomsection
\addcontentsline{toc}{section}{References}
\section*{References}


\printbibliography[
    heading = subbibintoc,
    type=article,
    title={Articles scientifiques}]
    
\printbibliography[
    heading = subbibintoc,
    type=online,
    title={Ressources en lignes}]
    
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\phantomsection
\addcontentsline{toc}{section}{Appendices}
\begin{appendix}

\section{Theoretical complements}

\begin{multicols}{2}

\subsection{Latent variables as a solution to the curse of dimensionality} \label{app:latent}

In this section, we provide substance to the first step of the derivation of a tractable objective function, taking roots in our overall goal of estimating the conditional distribution $p(x|y)$. We start with the joint distribution\footnote{We abusively call distribution the density function with respect to the correct mix of Lebesgue and discrete measures. Indeed, recall that $x$ is continuous, whereas $y$ and $z$ are discrete variables. We also abuse notation, not distinguishing the random vectors from their realisations, as is common in Bayesian or machine-learning literature.}: $$p(x,y)$$
Note that, abstracting from computational issue, we \textit{could} in theory retrieve the conditional distribution: $$p(x|y)=\frac{p(x,y)}{p(y)}=\frac{p(x,y)}{\int_x p(x,y)dx} $$
Unfortunately both estimating $p(x,y)$ or computing $\int_x p(x,y)dx$ is impossible due to the \textbf{curse of dimensionality}.

The \textbf{curse of dimensionality} is the idea that in high dimensions, the Euclidean space becomes sparser and sparser : observation become further apart from each other and more homogeneously distributed, which makes the very concept of distance meaningless. Alas, distribution estimation relies heavily of the notion of distance. Schematically, if $d$ is the space dimension, reducing uncertainty by a factor $k$ around a distribution $p$ requires an increase of the data set size by a factor of $k^d$. Such a collection is prohibitive for large $d$s !

In our case, the curse hits in two ways. The first blow comes when we want to estimate $p(x,y)$, since $x$ and $y$ both live in a high-dimensional space. The second blow comes when we want to estimate the integral along $x$. (Recall that even for small square images of size $\si$, the actual image space is of size $\di=3\si^2$.)

One solution, retained by \citet{zeroshot}, is to introduce an arbitrary low-dimensional \textbf{latent variable} $z$ to alleviate the curse. By definition of a conditional distribution, we have:
$$p(x,y,z)=p(x,y | z) p(z)$$
How does it help? First note that this equation stays true irrespective of the exact distribution $p(z)$. This means that we can chose the distribution to suit our needs. In particular, we may chose any independence assumption between $x$ and $(x,y)$ that leads to tractable computations! Then note that if we chose $z$ so that its dimension be orders of magnitude smaller than $x$'s ($\dl\ll\di$), integrating along $z$ is in principle much easier than integrating over $x$. Thus, it is in theory easier to compute $p(x,y)$ from $p(x,y | z)$ than computing $p(x|y)$ from $p(x,y)$. Specifically:
$$p(x,y)=\int_z p(x,y | z) p(z) dz$$
Finally, note that any expression that can be expressed as $\int_z f(z) p(z) dz=\mathbb{E}_z f(z)$ can be estimated through Monte-Carlo simulation by simulating $n_\text{sim}$ draws from the $z$ distribution, something that will turn out handy for estimating the gradient of the objective function in appendix \ref{app:estimation}. Indeed since any distribution can be chosen, we might as well chose one that is convenient to simulate ! Writing $z_i$ the $i$th draw from $p(z)$, we indeed have:
$$\int_z f(z) p(z) dz=\mathbb{E}_z f(z)\simeq \frac{1}{n_\text{sim}}\sum_{i=1}^{n_\text{sim}} f(z_i)$$

\end{multicols}

\subsection{\gls{elbo} derivation} \label{app:latent}

Here is the derivation of the \glslink{elbo}{evidence lower bound (ELBO)} in the case of one image-text observation $(x,y)$:

\begin{align*}
l_{x,y}(\theta,\psi)&=\ln p_{\theta,\psi}(x,y)\\
&=\ln \int_{z|x \sim h_\phi} p_{\theta,\psi}(x,y,z) dz  \quad \text{(integrating over the latent $z$)} \\
&=\ln \int_{z|x \sim h_\phi} f_{\theta}(x|z)g_\psi(y,z)\times\frac{h_\phi(z|x)}{h_\phi(z|x)} dz \\
&=\ln \mathbb{E}_{z|x \sim h_\phi} \Big[ \frac{f_{\theta}(x|z)}{h_\phi(z|x)}g_\psi(y,z)  \Big]\\
&\geqslant  \mathbb{E}_{z|x \sim h_\phi} \ln \Big[ \frac{f_{\theta}(x|z)}{h_\phi(z|x)}g_\psi(y,z)  \Big] \quad \text{(Jensen's inequality)} \\
& = \mathbb{E}_{z|x \sim h_\phi} \Big[ \ln f_{\theta}(x|z) - \ln h_\phi(z|x) + \ln g_\psi(y,z)  \Big] \\
& = \mathbb{E}_{z|x \sim h_\phi} \big[ \ln f_{\theta}(x|z) \big]- \mathbb{E}_{z|x \sim h_\phi}\Big[\ln h_\phi(z|x) - \ln g_\psi(y,z)  \Big] \\
& = \mathbb{E}_{z|x \sim h_\phi} \big[ \ln f_{\theta}(x|z) \big]-\KL \big(h_\phi \| g_\psi(y,\cdot) \big) \equiv \underline{\mathscr{l}}_{x,y}(\theta,\psi,\phi)
\end{align*}

... where $\KL(\cdot\|\cdot)$ represents the Kulback-Leibler divergence.

\begin{multicols}{2}

% \subsection{Reasons to believe in the tightness of the evidence lower bound} \label{app:tightness}

\subsection{Derivative of the evidence lower bound} \label{app:elbderivative}

1. derivation is "easy" since we can split up components

2. its is easier with theta and psi ; we can use monte-carlo

3. but there are problems with discrete variables

4. and there are more problems $z$

maximising the ELB through numerical methods means computing the exact derivatives or estimating it. Since the distribution of the latent variable $z$ does not depend $\theta$:

\begin{align*}
\nabla_\theta\elb_{\theta,\phi}(x) & = \nabla_\theta \esp_{z|x \sim q_\phi} \Big[ \; \ln  \pt(x, z) -\ln \qp(z \mid x)  \; \Big] \\
& = \esp_{z|x \sim q_\phi} \Big[ \; \nabla_\theta \ln \pt(x, z) - \underbrace{ \nabla_\theta \ln \qp(z \mid x) }_{= 0} \; \Big] \\
& = \esp_{z|x \sim q_\phi} \big[ \; \nabla_\theta \ln \pt(x, z)\big] \\
\end{align*}

What remains can be estimated without bias by the empirical mean on a draw of $z$ realisations taken from the distribution $\qp$.
However, since the distribution of $z$ depends on $\phi$, the same can not be said from $\nabla_\phi\elb_{\theta,\phi}(x)$, since one can no more interchange the derivation with the expectation.

The traditional computational work-around, dubbed the \textbf{reparametrisation trick}, consists in making yet an other assumption, namely that $z$ can be expressed as:
$$z=r(\varepsilon, \phi, x)$$
... where $\varepsilon$ is a random variable that does \textit{not} depend on $\phi$ and where $r$ is some function that happens to have (or is designed so to have) that $\partial r_i / \partial \epsilon_j$ is defined and tractable. (Here, $r_i$ represents the $i$-th component of $r$, and $\varepsilon_j$ the $j$-th component of $\varepsilon$.) For instance, assuming $z$ is of dimension 1, if $z\sim \mathcal{N}(\mu,\sigma^2)$ then define $z=r(\varepsilon, \phi, x)=\mu+\sigma \varepsilon$ with $\varepsilon\sim\mathcal{N}(0,1)$. The nice thing is that now, the expectation is defined with respect to $\varepsilon$, which is independent of $\phi$, so that now we can interchange the derivation with the expectation:

\begin{align*}
\nabla_\phi\elb_{\theta,\phi}(x) & = \nabla_\phi \esp_{z|x \sim q_\phi} \Big[ \; \ln  \pt(x, z) -\ln \qp(z \mid x)  \; \Big] \\
& = \nabla_\phi \esp_{\varepsilon} \Big[ \; \ln  \pt(x, r(\varepsilon,\phi,x)) -\ln \qp(r(\varepsilon,\phi,x) \mid x)  \; \Big] \\
& = \esp_{\varepsilon} \Big[ \; \nabla_\phi \ln  \pt(x, r(\varepsilon,\phi,x)) - \nabla_\phi \ln \qp(r(\varepsilon,\phi,x) \mid x)  \; \Big]
\end{align*}

... which actually can be estimated by simulating draws from $\varepsilon$'s distribution.

Unfortunately for us here, the authors prefer to suppose the latent variables as discrete, in order to leverage a discrete autoregressive modelisation of text (discrete) and the newly-encoded image tokens. But \enquote{as $q_{\psi}$ is a discrete distribution, and we cannot use the reparameterization gradient to maximize it.} \textcolor{red}{[POURQUOI LE FAIT QUE LA DISTRIBUTION SOIT DISCRÈTE EMPÊCHE L'ASTUCE?]} Instead, the opt for 

\subsection{Estimation procedure} \label{app:estimation}

At the end, the estimation (optimisation) procedure performed by \cite{zeroshot} looks like the following:

\newcommand{\zn}{\mathbf{z}_n}

\begin{enumerate}
    \item Initialise the distribution of $z$ as discrete-uniform, draw random parameters $\theta_0$,  $\phi_0$ and $\psi_0$. Define a held-out sample $(\bar{\mathbf{x}},\bar{\mathbf{y}})$ used for testing and initialise meta-parameters learning rate $r_0$, temperature $\tau_0$, absolute precision $\delta$ and patience $\Delta t$.
    \item \textbf{(Stage 1)} For each step $t>0$, update $\theta_t$ and $\phi_t$ as following. (The process does not use the captions $\mathbf{y}$ thanks to our independence assumptions.) Stop when $\underline{\mathscr{l}}_{\bar{\mathbf{x}}}(\theta_t,\phi_t)$, the objective function computed on the held-out sample, does not increase more than $\delta$ after $\Delta t$ steps. Let's note $t_1$ this maximal number of steps.
    \begin{enumerate}
    \item Take a random\footnote{In practice, no machine-learning algorithm uses true random batches, as the stochastic optimisation would require, but rather randomly sorts all statistical units, runs through the stack in order, then reshuffles when it reaches the bottom and starts back from the start.} batch $\xn$ of $n$ images\footnote{Since we are dealing with images, it is in practice almost impossible to have $n$ images loaded simultaneously into memory. Most implementation thus proceed with mini-batches of size $n^\prime<n$, and accumulate temporary results until the full batch has been processed. Indeed it is a bad idea to use a low $n$, since stochastic gradient descent rests on the idea of law of large number for the gradient estimation.}
    \item For each image, generate $k_1$ independent draws from the distribution $z_i|x_i\sim h_{\phi,x}$. You obtain $k_1n$ compressed images $z_{ij}$.
    \item For each compressed image, generate $k_2$ independent draws from the distribution $x_j|z_j\sim f_{\theta,z_j}$. You obtain $k_1k_2n$ reconstructed images $\hat x_{ijk}$.
    \item Use automatic differentiation to compute the derivative of the lower bound: \begin{align*}
    \underline{\mathscr{l}}_{x_i}(\theta_{t},\phi_{t}) & \equiv \mathbb{E}_{z_i|x_i} \Big[\ln f_{\theta_t,z_i}(x_i) -\ln h_{\phi_t,x_i}(z_i)\Big] \\ & \simeq \frac{1}{k_1k_2}\sum_{j=1}^{k_1}\sum_{k=1}^{k_2} f_{ \theta_t,z_{ij}}( \hat{x}_{ijk} )-\frac{1}{k_1}\sum_{j=1}^{k_1} h_{\phi_t,x_i}(z_{ij})
    \end{align*}
    We can safely ignore the other components of the ELBO since they are constant with respect to $\theta$ and $\phi$.
    \item Move parameters according to the estimated gradient. For the sake of simplicity, let's just consider the following trivial update: $$(\theta_{t+1},\phi_{t+1})=(\theta_{t},\phi_{t})+r_t\hat{\nabla}\underline{\mathscr{l}}_{\mathbf{x}_n}(\theta_{t},\phi_{t})$$
    (The real updating scheme used by \citeauthor{zeroshot}, known as AdamW, is more complicated.)
    \item Update learning rate $r_t$ and temperature $\tau_t$ according to some planned out scheme.
    \end{enumerate}
    
    \item \textbf{(Stage 2)} Set $\psi_{t_1}=\psi_0$, $r_{t_1}=r_0$. For each step $t>t_{1}$, update $\psi_t$ as following. Stop when $\underline{\mathscr{l}}_{\bar{\mathbf{x}},\bar{\mathbf{y}}}(\psi_t)$, the objective function computed on the held-out sample, does not increase more than $\delta$ after $\Delta t$ steps.  Let's note $t_2$ this maximal number of steps.
    
    \begin{enumerate}
    \item Take a random batch $(\xn,\yn)$ of $n$ image-caption pairs
    \item For each image, generate $k_1$ independent draws from the distribution $z_i\sim h_{\phi_{t_1},x_i}$. You obtain $k_1n$ compressed images $z_{ij}$.
    \item Use automatic differentiation to compute the derivative of the lower bound: \begin{align*}
    \underline{\mathscr{l}}_{x_i,y_i}(\psi_t) & \equiv \mathbb{E}_{z_i|x_i} \Big[\ln g_{\psi_t}(y,z)\Big]
    \end{align*}
    We can safely ignore the other components of ELBO which are constant with respect to $\psi$.
    \item Move parameters according to the estimated gradient, which for the sake of simplicity we present as: $$\psi_{t+1}=\psi_{t}+r_t\hat{\nabla}\underline{\mathscr{l}}_{\xn,\yn}(\theta_{t},\phi_{t})$$
    \item Update the learning rate $r_t$.
    \end{enumerate}
    \item The estimation is finished. Set $(\hat \theta, \hat \psi, \hat \phi)=(\theta_{t_1}, \psi_{t_2}, \phi_{t_1})$
\end{enumerate}

\end{multicols}

\section{More on CLIP} \label{app:more-CLIP}

CLIP stands for "Constrastive Language Pre-Training." This model is a neural network that was developed in order to associate to an image the best description among several possibilities. The goal of this model is to be able to achieve this association in a non-specific way (zero-shot learning), unlike neural networks trained for recognition in a particular field. In order to be as generalist as possible, the model was trained on 400 millions of image-text pairs. 

The input of the model is not exactly a set of images and texts. Instead, the text is going through a text encoder and the images are passing through an image encoder. The vectors in the condensed space (or embedding space) obtained after this are integrated in matrix, with all the vectors for each piece of text and each image.

At training time, the model receives NxN images-texts pairs (in the matrix described before) of which there are Nx1 correct pairs and Nx(N-1) incorrect pairs. Its goal is to maximise a measure of similarity between the correct pairs and to minimise that of the incorrect pairs. 

This learning of the notion of similarity allows to make accurate predictions on data outside of the training data. The quality of the encoders is then primordial, for they determine the vector representation of each piece of text and image. Thus, we can say that the quality of the model relies largely on that of the condensed space. 

At inference time, we give CLIP a set of texts and an image, and CLIP computes the similarity scores given the data provided by the encoders and the training weights of the model. The similarities obtained can be normalised in order to have a distribution of probabilities over the possible texts. The piece of text that receives the best score is considered to be the prediction of the model. It is important to note that the way the text are presented to CLIP has an importance: we can obtain better results by integrating the names of the different categories in a sentence. For example, given we want to predict of animal on a picture, it is better to propose as descriptions "a photo of cat/dog/elephant" instead of only using the name of the animal as a category. 

CLIP can be used for other tasks than classification: it is possible not to normalise the scores of similarity and instead to use the raw score corresponding to the scalar product of the embedding. 

The authors of the model note that despite the generality of the model, its performances remain of poor quality on data radically different from the training ones. Thus, CLIP does not really resolve the lack of generalisation abilities of this type of model, but gets around it using a generalist type of learning and a large volume of data. 
\textcolor{red}{section ayant disparu}

\section{Empirical results}

\subsection{CLIP}

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{Probability_scoring_clip.png}
    \caption{CLIP probability score on a generation of 10 images}
    \label{fig:clip_proba_exemple}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{A mountain behind a garden highest.png}
    \caption{The best image according to CLIP}
    \label{fig:clip_image_exemple}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{humanvsclip.png}
    \caption{The results of the Humans vs CLIP experiment}
    \label{fig:Humans vs CLIP}
\end{figure}


\subsection{Generation examples}

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a blue and yellow ball.jpeg}
  \caption{a blue and yellow ball}
  \label{fig:a blue and yellow ball.}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a blue tree in a circle.jpeg}
  \caption{a blue tree in a circle}
  \label{fig:a blue tree in a circle}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a cube.jpeg}
  \caption{a cube}
  \label{fig:a cube}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a green circle.jpeg}
  \caption{a green circle}
  \label{fig:a green circle}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a red and blue tree.jpeg}
  \caption{a photo of a red and blue tree}
  \label{fig:a photo of a red and blue tree}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a table.jpeg}
  \caption{a photo of a table}
  \label{fig:a photo of a table}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a small heart.jpeg}
  \caption{a small heart}
  \label{fig:a small heart}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{black and white triangle.jpeg}
  \caption{black and white triangle}
  \label{fig:black and white triangle}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{grass ball.jpeg}
  \caption{grass ball}
  \label{fig:grass ball}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{steel ball.jpeg}
  \caption{steel ball}
  \label{fig:steel ball}
\end{subfigure}
\caption{examples of simple generations}
\label{fig:first_generations}
\end{figure}


\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a baseball game.jpeg}
  \caption{a photo of a baseball game}
  \label{fig:a photo of a baseball game}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a bedroom.jpeg}
  \caption{a photo of a bedroom}
  \label{fig:a photo of a bedroom}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a man.jpeg}
  \caption{a photo of a man}
  \label{fig:a photo of a man}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a mountain behind a beach.jpeg}
  \caption{a photo of a mountain behind a beach}
  \label{fig:a photo of a mountain behind a beach}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a mountain made of wood.jpeg}
  \caption{a photo of a mountain made of wood}
  \label{fig:a photo of a mountain made of wood}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of a park with red lights.jpeg}
  \caption{a photo of a park with red lights}
  \label{fig:a photo of a park with red lights}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of people in a city.jpeg}
  \caption{a photo of people in a city}
  \label{fig:a photo of people in a city}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of people in a park.jpeg}
  \caption{a photo of people in a park}
  \label{fig:a photo of people in a park}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a photo of the sea made of blood.jpeg}
  \caption{a photo of the sea made of blood}
  \label{fig:a photo of the sea made of blood}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{a picture of a yellow park.jpeg}
  \caption{a picture of a yellow park}
  \label{fig:a picture of a yellow park}
\end{subfigure}
\caption{examples of more complex generations}
\label{fig:complex_generations}
\end{figure}

\subsection{Example of strange images obtained through greedy decoding}
\label{app:greedy-images}

These two figures present images we obtained using greedy decoding with multiple beams. Other outputs were simply fully blank or black. We were not able to find an explanation for this strange behavior. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 1.0]{weird1.jpeg}
    \caption{First example of strange image generated when using numerous beams but no sampling}
    \label{fig:gen-weird1}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 1.0]{weird2.jpeg}
    \caption{Second example of strange image generated when using numerous beams but no sampling}
    \label{fig:gen-weird2}
\end{figure}

\subsection{Study of parameter's impact on generated images}
\label{app:other-gen-param}

The 3 next figures are presenting the results of variations in the parameters of generation for 3 different prompts.

Recall that the x-axis refers to the different set of parameters used for generating the images. The grey line indicates CLIP's score of the original image with respect to its text description. Such line is not present for the synthetic prompt because it is not associated with an image, for we created this prompt. 

The black dots are indicating the empirical mean for each set of parameter on the 20 replicates. The confidence bounds at 95\% were obtained using the quantiles of the Student distribution at 19 degrees of freedom. 
Finally, the stars and dots at the top of the picture indicates the result of the different tests. 
\begin{itemize}
    \item The black symbols refers to the test of comparison with the score of the original image (grey line), one star indicates significance at 5\%, two stars for 1\%.
    \item  The red symbols are referring to the test of comparison with the standard set of parameters. One star indicates significance at 5\%, two stars for 1\%, but it is not the case in our results. Two dots indicates significance at 10\%, one dot for 20\%. 
\end{itemize}
The prompt used in this experiment is readable on the the top of the figure.

Our conclusion is the same that the one presented in the main part of this report: for non-synthetic prompt, CLIP's score of the images generated are significantly lower than the score of the original image. For some images and some set of parameters, the mean is significantly different from the standard set of parameters. But considering the significance of the test is pretty low (the 5\% significance is attained only once), it does not seem that not using the standard set of parameters is useful. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 1.0]{bwhite-dog.png}
    \caption{Parameters variation for the prompt "A picture of a black and white dog is running in a grassy garden surrounded by a white fence."}
    \label{fig:gen-bwhite-dog}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 1.0]{young-female.png}
    \caption{Parameters variation for the prompt "A picture of a young female student performing a downward kick to break a board held by her Karate instructor"}
    \label{fig:gen-young-female}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[scale = 1.0]{sunset.png}
    \caption{Parameters variation for the synthetic prompt "A picture of a 
    sunset over snow-capped mountains"}
    \label{fig:gen-sunset}
\end{figure}


\subsection{Color analysis}


\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{red hist.png}
  \caption{Red difference histogram}
  \label{fig:red_hist}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{green hist.png}
  \caption{Green difference histogram}
  \label{fig:green_hist}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{blue hist.png}
  \caption{Blue difference histogram}
  \label{fig:blue_hist}
\end{subfigure}
\caption{Color difference histograms}
\label{fig:histograms}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{kmean image.png}
  \caption{A mini dallE image}
  \label{fig:sfig_kmean_image}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{kmean.png}
  \caption{The colors detected}
  \label{fig:sfig_kmean}
\end{subfigure}
\caption{Color extraction}
\label{fig:kmean}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{notebook_color.png}
    \caption{Extract from our notebook using our method to retrieve all green images from a small set of caption+image pairs where green appears in the caption"}
    \label{fig:nb_color}
\end{figure}

\section{Code}

\subsection{Text encoding with \glslink{gpt}{GPT-2}} \label{textencoding}

\mintinline{python}{from transformers import GPT2Tokenizer} \newline
\mintinline{python}{tokenizer = GPT2Tokenizer.from_pretrained('gpt2')} \newline
\mintinline{python}{vocab     = list(tokenizer.encoder.keys())} \newline 
\mintinline{python}{phrase    = "an illustration of a baby daikon radish in a tutu walking a dog"} \newline 
\mintinline{python}{for token in tokenizer.encode(phrase) :} \newline 
\mintinline{python}{    print(vocab[token])} \newline

\end{appendix}

\end{document}
