\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage[toc]{glossaries}
\usepackage[margin=1.2in]{geometry} 
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{footnotehyper}

\usepackage[autostyle]{csquotes} % citation en ligne
\usepackage[
    backend=biber,
    style=apa,
    natbib
  ]{biblatex}
  
\addbibresource{biblio.bib} % import biblio file

%%%%%%%%%%%%%%%%%%%%%%%%
% Images
%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\graphicspath{{img/}}



\title{Statistical and empirical analysis of text-to-image generation models}
\author{L. Houairi,
A. Katossky,
A. Philippe,
C. Thiounn }
\date{Mai 2022}

\begin{document}

\begin{titlepage}
\maketitle
\begin{center}
Synthesis 

ENSAE 2021 - 2022: Applied Statistics Project

\vspace{5mm}

Conducted under the supervision of Benjamin Muller and Matthieu Futeral-Peter (INRIA) and Guillaume Lecué (ENSAE)
\end{center}
\end{titlepage}



%%%%%%%%%%%%%%%%%%%%%%%%
% Le glossaire
%%%%%%%%%%%%%%%%%%%%%%%%
\newglossaryentry{dalle}
{
  name={DALL$\cdot$E},
  description={The text-to-image model studied in this report},
}
\newglossaryentry{dallemini}
{
  name={DALL$\cdot$E Mini},
  description={The down-scaled open replication of \gls{dalle}, by \citeauthor{wandbdallemini}.}
}
\newglossaryentry{dalletiny}
{
  name={DALL$\cdot$E Tiny},
  description={Our down-scaled replication of \gls{dallemini}.}
}



%%%%%%%%%%%%%%%%%%%%%%%%
% Corps de la synthèse
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\begin{multicols}{2}

Within the broad field of Machine Learning, the text-to-image field aims at building models able to generate compelling images based on a text description. This report synthesises our study of one of those models: \gls{dalle}, which was developed by OpenAI (\cite{zeroshot}). Figure \ref{fig:lamp} gives an example of what this model can achieve. 

This model is made of two fundamental bricks,  the Variational Auto-Encoder (VAE) and the transformer (\cite{attneed}). Because classical pictures are too high-dimensional, the VAE compresses the images into a smaller space in which they are represented as a set of metapixels called tokens; the goal being to preserve most of the information of the image despite the lower dimension. The transformer models the relationship between the text and the image tokens. It is able to recursively generate a sequence of image tokens based on a text prompt. Given a prompt, it produces the first metapixel of the image, then produces a metapixel taking into account the prompt and the previous pixel, and so on until an image is completed. The VAE can then be used to produce an image based on the set of metapixels. 

\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{a lamp in the shape of an orchid.png}
    \caption{A lamp in the shape of an orchid by \gls{dalle} (images taken from \cite{openaidalle}).}
    \label{fig:lamp}
\end{figure}

\Gls{dalle} is not fully available to the public. Therefore, we rely on \gls{dallemini} (\cite{wandbdallemini, dalleminigit}), an open-source implementation that tries to reproduce the results obtained by \gls{dalle}. The model architecture is not exactly the same, but the goal remains untouched, with results that we explore in our report, although it is agreed that the lower size of the model implies less impressive generations. Two examples of generation of \gls{dallemini} can be seen in figure \ref{fig:mini_dallE_exemples}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a picture of a yellow park.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a bedroom.jpeg}
    \end{subfigure}
    \caption{Images produced by mini \gls{dalle} : A picture of a yellow park, A photo of a bedroom (images taken from \cite{openaidalle}).}
    \label{fig:mini_dallE_exemples}
\end{figure}

One of the challenges in training models capable of generating images is to evaluate the quality of their outputs. How can one quantify the adequacy of an image with a text description? To deal with this issue, we rely on another model developed by OpenAI, CLIP  \footnote{CLIP stands for Contrastive Language Pre-Training.}(\cite{learntransf}). CLIP is capable of giving an adequacy score for an image-text pair (absolute number). If we want to find the best text description for a given image, the absolute scores can be turned into a distribution of probability, using a softmax function. 

This document is a synthesis of a longer report. Here, we present our evaluation of CLIP and the generating capabilities of \gls{dallemini}. Then, we give an overview of an attempt to create our small version of \gls{dalle} and show the obtained results. For more details, especially on the mathematics and technical background of \gls{dalle}, please refer to our complete report. 

\end{multicols}

\section{Models' capabilities}

\begin{multicols}{2}

\subsection{CLIP's evaluation}

After taking a human look at the images generated, we aim to rely on CLIP for estimating the quality of the output images to make more quantitative analysis of \gls{dallemini} using several image datasets. Therefore we start by assessing whether this model is functioning properly. 

Firstly, we use CLIP on the Flickr test set that provides hundreds of prompt-image pairs. Thanks to \gls{dallemini} we are able to generate another image for each prompt available in the dataset. We use CLIP to compute the adequacy of each image (original and generated) with the corresponding prompt. We conclude that CLIP's scores of the generated images are lower than those of the original images, which seems reasonable according to us.

In another experiment, we compare the adequacy rating of CLIP with our own on a selected set of texts and pictures. In short, we select 100 prompt-image pairs in the dataset, then create two alternative prompts: a description close to the image and a piece of text that has nothing to do with it. We then use CLIP to compute the adequacy of the images with each of the three prompts. We find that when CLIP gives a high adequacy score, the human notation is in agreement with it. In some cases, adequate images according to human notation received a low CLIP score. This is mainly due to the fact that we use the scores as a probability distribution in this experiment: if one of the text descriptions is considered very adequate by CLIP, the softmax will push its probability towards 1 and the probability of the other possible description towards 0, which is not a natural method for humans. 

Nevertheless, this experiment allows us to be confident in CLIP scoring, and to use it instead of our eyes in quantitative experiments involving lots of images. 

\subsection{Assessment of \gls{dallemini}}

Our first approach for evaluating the capabilities of \gls{dallemini} is to try different types of text inputs and see if the generated images correspond to these prompts. We experiment with basic geometric forms and objects, textures, colours, quantifiers and size. Two examples can be seen in figure \ref{fig:mini_dallE_exemples_bad}. Overall, the model seems to manage well the colours and textures but not much the rest. It is also very sensitive to the phrasing of the text description: adding "an image of ..." at the beginning of the description greatly increases the quality of the image, probably due to the high frequency of such description prefix in the training set. We also find that \gls{dallemini} can generate compelling landscapes (see figure \ref{fig:mini_dallE_exemples_good}). Some images are related to the text prompt but somewhat surprising: figure \ref{fig:mini_dallE_exemples_fun}.

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a computer.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a table.jpeg}
    \end{subfigure}
    \caption{Bad images produced by mini \gls{dallemini} : A photo of a computer, A photo of a table} (images generated by us).
    \label{fig:mini_dallE_exemples_bad}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a mountain behind a beach.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a blue tree in a circle.jpeg}
    \end{subfigure}
    \caption{Impressive images produced by mini \gls{dallemini} : A photo of a mountain behind a beach, A blue tree in a circle} (images generated by us).
    \label{fig:mini_dallE_exemples_good}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{circle of golden coins.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{A picture of a red bus in a city.jpeg}
    \end{subfigure}
    \caption{Funny images produced by mini \gls{dallemini} : Circle of golden coins, A picture of a red bus in a city} (images generated by us).
    \label{fig:mini_dallE_exemples_fun}
\end{figure}

\subsection{Colour analysis}

We realise that the colour instructions of the prompts are relatively respected by \gls{dallemini} and want to take a closer look. Here, we analyse how \gls{dallemini} manages the colours in more detail. 

We want to know two things about the images generated by \gls{dallemini}:
\begin{itemize}
    \item If the generated images are following the same colour distribution as the images from the training datasets. We plan to do so by comparing the RGB colour histograms. This is an important observation for tracing generation bias that may exist.
    \item To what extent \gls{dallemini} correctly takes into account the colours present in the text description when it generates a picture. 
\end{itemize}

As for CLIP's evaluation, we use part of the Flickr dataset as a set of real images, and we generate images based on the same prompts. Then, for red, green and blue, (colours are coded in RGB) we plot the corresponding colour histogram. (It allows to see the level of activation of the pixels for each colour).
Visually, we find that the RGB colour histograms are close. For each colour, we then perform a Kolmogorov–Smirnov test to compare the histogram of the real images with the histogram of the generated ones. All three tests lead to a rejection of the null hypothesis of equality of the distributions at the 5\% level. So, even though the colour distribution closely follows the original, it seems that the images generated by \gls{dallemini} present a bias. 

To assess if \gls{dallemini} correctly takes into account the colours in the text, we design a few prompts mentioning a colour instruction ("a picture of a green bus"). We generate images based on these prompts for multiple colours. We also have a control group, the same prompts without colour instructions ("a picture of a bus"). We then retrieve the colours present in each image, and compare the frequency of presence of some colours depending on whether they are required by the text description. The difference in frequency between the colour group and the control group in percentage points gives us an indicator of the performance of the model regarding colour management. 

Overall, we conclude that \gls{dallemini} performs well for drawing colours. A detailed analysis for each colour is available in the main report, because the model performance really depends on the colour. Black and white are present in every picture, even if such colour is not asked for. Also, some colours could be over-represented  because of the prompt (when asking for a sea, the model does not need to be told to draw it blue). It also seems that some colours will appear more often because of their high frequency in the dataset. The main added value of this analysis is the design of a colour recognition tool. Integrating somehow such tool to CLIP could be interesting,  because we notice that CLIP does not always take into account the respect of the colour instruction in its scoring. 

\end{multicols}


\section{An attempt to create a \gls{dalle} of our own}

\begin{multicols}{2}

For our implementation of \gls{dalle}, in order to train a complex neural network that can process a text input and output an image related to the input, we use a specific neural network architecture called a transformer. In order to build our model, we reuse multiple components from other models instead of starting from scratch.
Firstly, we reuse the already-trained \gls{dallemini}'s image processing neural network (VQGAN, \cite{tamingtransfo}) to transform images into image-tokens and vice-versa. Secondly, we start from a pretrained Bart transformer and adapt it to output image tokens.

With the 2017 MSCOCO dataset, we train our model by feeding as input the image-caption pairs and as output the target image tokens. We compute the difference between the output and the target, called cross-entropy loss. Thanks to that computation, our model is able to correct its parameters (weights) to make better predictions, through a mechanism called backpropagation. We train our model on the training dataset and check the loss on the testing dataset. When we stop our training at the local minimum test loss, our model fails to render adequate images. By overfitting the training dataset, our model can generate newer images but fails to take into account the initial prompt (see figure \ref{fig:tinydalle-images-generated}).

Apart from being unable to replicate \gls{dalle}, we run different experiments that show many interesting results. Firstly, fixing a set of parameters, called frozen parameters, during training can accelerate the training. Indeed, instead of correcting all parameters, the model can focus on correcting a subset of parameters. Hence, there are fewer parameters to correct and thus fewer computations. Moreover, since there are fewer parameters to correct which are stored in the GPUs' memory, the GPUs can fit more data in its memory.
Furthermore, instead of letting our model correct its parameters at each batch, we can accumulate corrections (a procedure called accumulated gradient), and apply these aggregated corrections less frequently. This technique results in fewer back and forth loss corrections but ultimately leads to a steady plateau for both training loss and test loss.

\end{multicols}
\begin{figure}[h!]
    
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_red_mountain.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_yellow_sky.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_beach_covered_in_snow.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_red_mountain_with_a_red_background.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_sea.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_sky.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_view_of_a_beach_covered_in_snow.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_view_of_a_yellow_sky.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_giraffe.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_giraffe.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_cube.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_underfit.jpg}}
    
    
    
    
    \caption{ Images generated by \gls{dalletiny}: (a) A photo of a red mountain. (b) A photo of a yellow sky. (c) A picture of a beach covered in snow. (d) A picture of a red mountain with a red background. (e) A sea. (f) A sky. (g) A view of a beach covered in snow. (h) A view of a yellow sky. (i) A giraffe. (j) A photo of a giraffe (k) A picture of a cube. (l) [any caption with min test loss] }
    \label{fig:tinydalle-images-generated}
    
\end{figure}

\pagebreak

\section*{Minimal Bibliography}

\printbibliography[
    heading = subbibintoc,
    type=article,
    title={Scientific papers}]
    
\printbibliography[
    heading = subbibintoc,
    type=online,
    title={Online ressources}]

\end{document}
