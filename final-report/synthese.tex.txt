\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage[toc]{glossaries}
\usepackage[margin=1.2in]{geometry} 
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{footnotehyper}

%%%%%%%%%%%%%%%%%%%%%%%%
% Images
%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\graphicspath{{img/}}



\title{Statistical and empirical analysis of text-to-image generation models}
\author{L. Houairi,
A. Katossky,
A. Philippe,
C. Thiounn }
\date{Mai 2022}

\begin{document}

\begin{titlepage}
\maketitle
\begin{center}
Synthesis 

ENSAE 2021 - 2022: Applied Statistics Project

\vspace{5mm}

Conducted under the supervision of Benjamin Muller and Matthieu Futeral-Peter (INRIA) and Guillaume Lecué (ENSAE)
\end{center}
\end{titlepage}



%%%%%%%%%%%%%%%%%%%%%%%%
% Le glossaire
%%%%%%%%%%%%%%%%%%%%%%%%
\newglossaryentry{dalle}
{
  name={DALL$\cdot$E},
  description={The text-to-image model studied in this report},
}
\newglossaryentry{dallemini}
{
  name={DALL$\cdot$E Mini},
  description={The down-scaled open replication of \gls{dalle}, by \citeauthor{wandbdallemini}.}
}
\newglossaryentry{dalletiny}
{
  name={DALL$\cdot$E Tiny},
  description={Our down-scaled replication of \gls{dallemini}.}
}



%%%%%%%%%%%%%%%%%%%%%%%%
% Corps de la synthèse
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\begin{multicols}{2}

Within the broad field of Machine Learning, the text-to-image field aims at building models able to generate compelling images based on a text description. This report synthesises our study of one of those models \gls{dalle}, which was developed by OpenAI. 

This model is made of two fundamental bricks,  the Variational Auto-Encoder (VAE) and the transformer. Because classical pictures are too high-dimensional, the first brick is used to compress the images into a smaller space in which they are represented as a set of metapixels called tokens; the goal being to preserve most of the information of the image despite the lower dimension. The second one models the relationship between the text and the image tokens. It is able to recursively generate a sequence of image tokens based on a text prompt. Given a prompt, it produces one pixel of an image, and then produces a second pixel taking into account the prompt and the previous pixel, and so on until an image is completed.
\begin{figure}[H]
    \centering
    \includegraphics[scale = 0.6]{a lamp in the shape of an orchid.png}
    \caption{A lamp in the shape of an orchid by DALL.E}
    \label{fig:lamp}
\end{figure}

\Gls{dalle} is not fully available to the public. Therefore, we relied on \gls{dallemini}, an open-source implementation that tries to reproduce the results obtained by \gls{dalle}. The model architecture is not exactly the same, but the goal remains untouched, with results that we explore in our report, although it is agreed that the lower size of the model implies less impressive generations.
\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}color
    \includegraphics[width=.4\linewidth]{a picture of a yellow park.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a bedroom.jpeg}
    \end{subfigure}
    \caption{Images produced by mini DALL.E : A picture of a yellow park, A photo of a bedroom}
    \label{fig:mini_dallE_exemples}
\end{figure}

One of the challenges in training models capable of generating images is to evaluate the quality of their outputs. How can one quantify the adequacy of an image with a text description? To deal with this issue, we rely on another model developed by OpenAI, CLIP  \footnote{CLIP stands for Contrastive Language Pre-Training.}. CLIP is capable of giving an adequacy score for an image-text pair (absolute number). If we want to find the best text description for a given image, the absolute scores can be turned into a distribution of probability, using a \textcolor{red}{softmax function (si je ne me trompe pas ?}.

This document is a synthesis of a longer report. Here, we present our evaluation of CLIP and the generating capabilities of \gls{dallemini}. Then, we explain how we tried to create our small version of \gls{dalle} and show the obtained results. For more details, especially on the mathematics and technical background of \gls{dalle}, please refer to our complete report. 

\end{multicols}

\section{Models' capabilities}

\begin{multicols}{2}

\subsection{CLIP's evaluation}

After taking a human look at the images generated, we aimed to rely on CLIP for estimating the quality of the output images to make more quantitative analysis of \gls{dallemini} using several image datasets. Therefore we started by assessing whether this model was functioning properly. 

Firstly, we use CLIP on the Flickr test set that provides hundreds of prompt-image pairs. Thanks to \gls{dallemini} we are able to generate another image for each prompt available in the dataset. We use CLIP to compute the adequacy of each image (original and generated) with the corresponding prompt. We conclude that CLIP's scores of the generated images are lower than those of the original images, which seems reasonable according to us.

In another experiment, we compare the adequacy rating of CLIP with our own on a selected set of texts and pictures. In short, we select 100 prompt-image pairs in the dataset, then create two alternative prompts: a description close to the image and a piece of text that has nothing to do with it. We then use CLIP to compute the adequacy of the images with each of the three prompts. We find that when CLIP gives a high adequacy score, the human notation is in agreement with it. In some cases, adequate images according to human notation received a low CLIP score. This is mainly due to the fact we use the scores as a probability distribution in this experiment: if one of the text descriptions is considered very adequate by CLIP, the softmax will push its probability towards 1 and the probability of the other possible description towards 0, which is not a natural method for a human. 

Nevertheless, this experiment allows us to be confident in CLIP scoring, and to use it instead of our eyes in quantitative experiments involving lots of images. 

\subsection{Assessment of \gls{dallemini}}

Our first approach for evaluating the capabilities of \gls{dallemini} is to try different types of text inputs and see if the generated images correspond to these prompts. We experiment with basic geometric forms, textures, colours, quantifiers and size. Overall, the model seems to manage well the colours and textures but not much the rest. It is also very sensitive to the phrasing of the text description: adding "an image of ..." at the beginning of the description greatly increases the quality of the image, probably due to the high frequency of such description prefix in the training set. We also found that \gls{dallemini} can generate compelling landscapes.  

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a computer.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a table.jpeg}
    \end{subfigure}
    \caption{Bad images produced by mini \gls{dallemini} : A photo of a computer, A photo of a table}
    \label{fig:mini_dallE_exemples_bad}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a photo of a mountain behind a beach.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{a blue tree in a circle.jpeg}
    \end{subfigure}
    \caption{Impressive images produced by mini \gls{dallemini} : A photo of a mountain behind a beach, A blue tree in a circle}
    \label{fig:mini_dallE_exemples_good}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{circle of golden coins.jpeg}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
    \includegraphics[width=.4\linewidth]{A picture of a red bus in a city.jpeg}
    \end{subfigure}
    \caption{Funny images produced by mini \gls{dallemini} : Circle of golden coins, A picture of a red bus in a city}
    \label{fig:mini_dallE_exemples_fun}
\end{figure}




\subsection{Colour analysis}

The assessment of \gls{dallemini} made us think that it would be relevant to tackle colour management in more detail. We realized that the colour instructions of the prompts were relatively respected and we want to take a closer look.

We want to know two things about the images generated by \gls{dallemini}
\begin{itemize}
    \item We want to know if the generated images follow the same colour distribution as the images from the training datasets. We plan to do so by comparing the RGB colour histograms. This is an important observation for tracing generation bias that may exist.
    \item We also want to estimate to what extent \gls{dallemini} correctly takes into account the colours present in the text description of the picture.
\end{itemize}

As for CLIP's evaluation, we use part of the Flickr dataset as a set of real images, and we generate images based on the same prompts. Then, for red, green and blue, (colours are coded in RGB) we plot the corresponding colour histogram. (It allows to see the level of activation of the pixels for each colour).
Visually, we find that the RGB colour histograms are close. For each colour, we then perform a Kolmogorov–Smirnov test to compare the histogram of the real images with the histogram of the generated ones. All three tests lead to a rejection of the null hypothesis of equality of the distributions at the 5\% level. So, even though the colour distribution closely follows the original, it seems that the images generated by \gls{dallemini} present a bias. 

To assess if \gls{dallemini} correctly takes into account the colours in the text, we design a few prompts mentioning a colour instruction ("a picture of a green bus"). We generate images based on these prompts for multiple colours. We also have a control group, the same prompts without colour instructions ("a picture of a bus"). We then retrieve the colours present in each image, and compare the frequency of presence of some colours depending on whether they are required by the text description. The difference in frequency between the 'red' group and the control group in percentage points gives us an indicator of the performance of the model.

Overall, we conclude that \gls{dallemini} performs well for drawing colours. In the report, you can see a detailed analysis of each colour, because the model performance really depends on the colour. Black and white are present in every picture, even if such colour is not asked for, and colours could be over-represented naturally depending on the prompt (when asking for a sea, the model does not need to be told to draw it blue). It also seems that some colours will appear more often because of their high frequency in the dataset. The main added value of this analysis is the design of a colour recognition tool that could be integrated into CLIP to strengthen its scoring, because we noticed that it does not always take into account the respect of the colour instruction in its scoring. 

\end{multicols}


\section{An attempt to create a \gls{dalle} of our own}

\begin{multicols}{2}

For our implementation of \gls{dalle}, in order to train a complex neural network that can process a text input and output an image related to the input, we use a specific neural network architecture called a transformer. We reuse multiple components for our model from other models instead of starting from scratch.
Firstly, we reuse the already-trained \gls{dallemini}'s image processing neural network (VQGAN) to transform images into image-tokens and vice-versa. Secondly, we start from a pretrained Bart transformer and adapt it to output image tokens.

With the 2017 MSCOCO dataset, we train our model by feeding as input the image-caption pairs and as output the target image tokens. We compute the difference between the output and the target, called cross-entropy loss. Thanks to that computation, our model is able to correct its parameters (weights) to make better predictions, through a mechanism called backpropagation. We train our model with the training dataset and check the loss with the testing dataset. When we stop our training at the local minimum test loss, our model fails to render adequate images. By overfitting the training dataset, our model can generate newer images but fails to take into account the initial prompt (see figure \ref{fig:tinydalle-images-generated}).

Apart from being unable to replicate \gls{dalle}, we run different experiments that show many interesting results. Firstly, fixing a set of parameters, called frozen parameters, during training can accelerate the training. Indeed, instead of correcting all parameters, the model can focus on correcting a subset of parameters. Hence, they are fewer parameters to correct and thus fewer computations. Moreover, since there are fewer parameters to correct which are stored in the GPUs' memory, the GPUs can fit more data into memory.
Furthermore, instead of letting our model correct its parameters at each batch, we can accumulate corrections, called accumulated gradient, and apply these aggregated corrections less frequently. This technique results in fewer back and forth loss corrections but ultimately leads to a steady plateau for both training loss and test loss.

\end{multicols}
\begin{figure}[h!]
    
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_red_mountain.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_yellow_sky.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_beach_covered_in_snow.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_red_mountain_with_a_red_background.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_sea.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_sky.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_view_of_a_beach_covered_in_snow.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_view_of_a_yellow_sky.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_giraffe.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_photo_of_a_giraffe.jpg}} 
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_a_picture_of_a_cube.jpg}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{tinydalle_underfit.jpg}}
    
    
    
    
    \caption{(a) A photo of a red mountain. (b) A photo of a yellow sky. (c) A picture of a beach covered in snow. (d) A picture of a red mountain with a red background. (e) A sea. (f) A sky. (g) A view of a beach covered in snow. (h) A view of a yellow sky. (i) A giraffe. (j) A photo of a giraffe (k) A picture of a cube. (l) [any caption with min test loss] }
    \label{fig:tinydalle-images-generated}
    

\end{figure}


\section{Conclusion}
\textcolor{red}{Je ne suis même pas sûr qu'elle soit nécessaire}
\begin{multicols}{2}

\end{multicols}

\end{document}
